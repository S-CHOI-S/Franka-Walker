{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# import mujoco\n",
        "# import gymnasium as gym\n",
        "# import numpy as np\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# import torch.nn.functional as F\n",
        "# from torch.distributions.normal import Normal\n",
        "\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# JOINT = 3\n",
        "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "# # Value\n",
        "# class ValueNetwork(nn.Module):\n",
        "#     def __init__(self, input_dim):\n",
        "#         super(ValueNetwork, self).__init__()\n",
        "#         self.fc1 = nn.Linear(input_dim, 128)\n",
        "#         self.fc2 = nn.Linear(128, 128)\n",
        "#         self.fc3 = nn.Linear(128, 1)\n",
        "        \n",
        "        \n",
        "        \n",
        "#     def forward(self, x):\n",
        "#         x = torch.relu(self.fc1(x))\n",
        "#         x = torch.relu(self.fc2(x))\n",
        "#         return self.fc3(x)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    \n",
        "# ''' Constraints '''\n",
        "# # joint angle constraint\n",
        "# def get_joint_angle_constraints(model):\n",
        "#     # print(mujoco.mj_name2id(model, JOINT, \"foot_joint\"))\n",
        "#     # print(model.jnt_range[5][0])\n",
        "#     return model.jnt_range[3:,:]\n",
        "\n",
        "# def compute_constraints(log_probs, actions, constraints):\n",
        "#     # Example constraint: log_probs of actions should be less than a threshold\n",
        "#     constraint_violation = torch.sum(torch.clamp(log_probs - constraints, min=0.0))\n",
        "#     return constraint_violation\n",
        "\n",
        "# def compute_cost(rewards, gamma=0.99):\n",
        "#     R = 0\n",
        "#     cost = []\n",
        "#     for r in rewards[::-1]:\n",
        "#         R = r + gamma * R\n",
        "#         cost.insert(0, R)\n",
        "#     return cost\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Policy (Actor)\n",
        "# class PolicyNetwork(nn.Module):\n",
        "#     def __init__(self, input_dim, output_dim, max_action):\n",
        "#         super(PolicyNetwork, self).__init__()\n",
        "#         self.fc1 = nn.Linear(input_dim, 128)\n",
        "#         self.fc2 = nn.Linear(128, 128)\n",
        "        \n",
        "        \n",
        "        \n",
        "#         self.mu = nn.Linear(128, output_dim)\n",
        "#         self.sigma = nn.Linear(128, output_dim)\n",
        "        \n",
        "#         self.max_action = torch.tensor(max_action, dtype=torch.float32, device=DEVICE).clone().detach()\n",
        "#         self.device = DEVICE\n",
        "        \n",
        "#     def forward(self, x):\n",
        "#         x = torch.relu(self.fc1(x))\n",
        "#         x = torch.relu(self.fc2(x))\n",
        "        \n",
        "#         mu = torch.tanh(self.mu(x)) * torch.tensor(self.max_action).to(self.device)\n",
        "#         sigma = F.softplus(self.sigma(x)) # \ucd9c\ub825\ub41c \ud45c\uc900\ud3b8\ucc28\uac12\uc5d0 \ub300\ud574\uc11c softplus \ud568\uc218\ub97c \uc801\uc6a9\ud574\uc11c \uc591\uc218\ub97c \ubcf4\uc7a5, \ud45c\uc900\ud3b8\ucc28\uac12 \uc870\uc808\n",
        "\n",
        "#         return mu, sigma\n",
        "\n",
        "#     def sample_action(self, state):\n",
        "#         mu, sigma = self.forward(state)\n",
        "#         distribution = Normal(mu, sigma)\n",
        "\n",
        "#         actions = distribution.sample()\n",
        "        \n",
        "#         action = torch.tanh(actions) * torch.tensor(self.max_action, dtype=torch.float32)\n",
        "        \n",
        "#         log_probs = distribution.log_prob(actions) # sampling\ub41c action\uc5d0 \ub300\ud55c log \ud655\ub960\uc744 \uacc4\uc0b0\n",
        "#         log_probs = log_probs.sum(1, keepdim=True) # \uac01 action\uc758 log \ud655\ub960\uc744 \ud569\uc0b0\ud558\uc5ec \ubc18\ud658\n",
        "\n",
        "#         return action, log_probs\n",
        "    \n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "# class Walker():\n",
        "#     def __init__(self, env=None):\n",
        "#         self.policy_net = PolicyNetwork(env.observation_space.shape[0], env.action_space.shape[0], env.action_space.high[0]).to(DEVICE)\n",
        "#         self.value_net  = ValueNetwork(env.observation_space.shape[0]).to(DEVICE)\n",
        "#         # self.critic_net = \n",
        "        \n",
        "#         self.optimizer_policy = optim.Adam(self.policy_net.parameters(), lr=1e-3)\n",
        "#         self.optimizer_value = optim.Adam(self.value_net.parameters(), lr=1e-3)\n",
        "        \n",
        "        \n",
        "#     def choose_action(self, state):\n",
        "#         state = torch.Tensor(np.array([state]).astype(np.float32)).to(DEVICE)\n",
        "#         action, action_probs = self.policy_net.sample_action(state)\n",
        "\n",
        "#         return action.cpu().detach().numpy().astype(np.float32)[0]\n",
        "    \n",
        "#     # advantage (Q - V)/ GAE (Generalized Advantage Estimation) \uae30\ubc18 \ubc29\uc2dd \uc0ac\uc6a9\n",
        "#     def compute_advantages(self, rewards, values, dones, gamma=0.99, tau=0.95):\n",
        "#         advantage = 0\n",
        "#         advantages = []\n",
        "#         for i in reversed(range(len(rewards))):\n",
        "#             td_error = rewards[i] + gamma * values[i + 1] * (1 - dones[i]) - values[i] # done = 1\uc774\uba74 episode\uac00 \uc644\ub8cc\n",
        "#             advantage = td_error + gamma * tau * (1 - dones[i]) * advantage\n",
        "#             advantages.insert(0, advantage)\n",
        "#         return advantages\n",
        "    \n",
        "#     def update(self, states, actions, rewards, next_states, dones): # constraints\ub3c4 \ucd94\uac00\ud574\uc918\uc57c \ud568\n",
        "#         states = torch.tensor(states, dtype=torch.float32).to(DEVICE)\n",
        "#         actions = torch.tensor(actions, dtype=torch.float32).to(DEVICE)\n",
        "#         rewards = torch.tensor(rewards, dtype=torch.float32).to(DEVICE)\n",
        "#         next_states = torch.tensor(next_states, dtype=torch.float32).to(DEVICE)\n",
        "#         dones = torch.tensor(dones, dtype=torch.float32).to(DEVICE)\n",
        "        \n",
        "#         # advantage \uacc4\uc0b0\n",
        "#         values = self.value_net(states).squeeze()\n",
        "#         next_values = self.value_net(next_states).squeeze()\n",
        "#         advantages = self.compute_advantages(rewards, values, dones)\n",
        "#         advantages = torch.tensor(advantages, dtype=torch.float32).to(DEVICE)\n",
        "        \n",
        "#         _, log_probs = self.policy_net.sample_action(states)\n",
        "#         policy_loss = -(log_probs * advantages).mean()\n",
        "        \n",
        "#         self.optimizer_policy.zero_grad()\n",
        "#         policy_loss.backward()\n",
        "#         self.optimizer_policy.step()\n",
        "        \n",
        "#         returns = values + advantages\n",
        "#         value_loss = nn.MSELoss()(values, returns)\n",
        "        \n",
        "#         self.optimizer_value.zero_grad()\n",
        "#         value_loss.backward()\n",
        "#         self.optimizer_value.step()\n",
        "\n",
        "#         # # Compute log probabilities\n",
        "#         # log_probs = torch.log(self.policy_net(states))\n",
        "        \n",
        "#         # # Compute cost and advantages\n",
        "#         # cost = compute_cost(rewards)\n",
        "#         # advantages = self.compute_advantages(rewards, self.value_net(states), dones)\n",
        "\n",
        "#         # # Compute policy loss with barrier function for constraints\n",
        "#         # policy_loss = -torch.mean(log_probs * advantages)\n",
        "#         # constraint_violation = compute_constraints(log_probs, actions, constraints)\n",
        "#         # barrier = 1.0 / constraint_violation\n",
        "        \n",
        "#         # # Combine losses\n",
        "#         # total_loss = policy_loss + barrier\n",
        "        \n",
        "#         # # Update policy network\n",
        "#         # self.optimizer_policy.zero_grad()\n",
        "#         # total_loss.backward()\n",
        "#         # self.optimizer_policy.step()\n",
        "\n",
        "#         # # Update value network\n",
        "#         # value_loss = nn.MSELoss()(self.value_net(states), torch.tensor(cost))\n",
        "#         # self.optimizer_value.zero_grad()\n",
        "#         # value_loss.backward()\n",
        "#         # self.optimizer_value.step()\n",
        "\n",
        "\n",
        "\n",
        "# MAX_EPISODES = 100000\n",
        "# MAX_STEPS = 1000\n",
        "\n",
        "\n",
        "# def main():\n",
        "#     # Walker2d-v4 \ud658\uacbd \ubd88\ub7ec\uc624\uae30\n",
        "#     env = gym.make('Walker2d-v4', render_mode='human')\n",
        "#     model = mujoco.MjModel.from_xml_path('../model/walker2d.xml')\n",
        "#     # data = mujoco.MjData(model)\n",
        "    \n",
        "#     # IPO Network\n",
        "#     walker = Walker(env)\n",
        "    \n",
        "#     # num_episodes = 1000\n",
        "#     gamma = 0.99\n",
        "#     tau = 0.95\n",
        "    \n",
        "#     state, _ = env.reset()\n",
        "    \n",
        "#     for episode in tqdm(range(MAX_EPISODES)):\n",
        "#         state, _ = env.reset()\n",
        "#         episode_reward = 0\n",
        "        \n",
        "#         states, actions, rewards, next_states, dones = [], [], [], [], []\n",
        "        \n",
        "#         for step in range(MAX_STEPS):\n",
        "#             done = False\n",
        "\n",
        "#             action = walker.choose_action(state)\n",
        "#             print(action) # [-0.8873158   0.81476843 -0.57397985  0.19199446  0.7000201  -0.09051505]\n",
        "#             print(\"=====================\")\n",
        "            \n",
        "#             next_state, reward, terminated, truncated, info = env.step(action)\n",
        "#             done = terminated or truncated\n",
        "            \n",
        "#             states.append(state)\n",
        "#             actions.append(action)\n",
        "#             rewards.append(reward)\n",
        "#             next_states.append(next_state)\n",
        "#             dones.append(done)\n",
        "            \n",
        "#             state = next_state\n",
        "#             episode_reward += reward\n",
        "            \n",
        "#             if done:\n",
        "#                 break\n",
        "        \n",
        "#         if episode % 300 == 0:\n",
        "#             # walker.save_models()\n",
        "#             pass\n",
        "        \n",
        "#         walker.update(states, actions, rewards, next_states, dones)\n",
        "        \n",
        "#         # joint_name_to_id = {name: i for i, name in enumerate(model.joint_names)}\n",
        "        \n",
        "#         # def get_joint_constraints(joint_name):\n",
        "#         #     joint_id = joint_name_to_id[joint_name]\n",
        "#         #     joint_range = model.jnt_range[joint_id]\n",
        "#         #     return joint_range\n",
        "        \n",
        "#         # for joint_name in joint_name_to_id.keys():\n",
        "#         #     joint_range = get_joint_constraints(joint_name)\n",
        "#         #     print(f'Joint: {joint_name}, Range: {joint_range}')\n",
        "\n",
        "#         # thigh_joint_range = get_joint_constraints('thigh_joint')\n",
        "#         # print(f'Thigh Joint Range: {thigh_joint_range}')\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions.normal import Normal\n",
        "from tqdm import tqdm\n",
        "from collections import deque\n",
        "\n",
        "RED = \"\\033[31m\"\n",
        "GREEN = \"\\033[32m\"\n",
        "YELLOW = \"\\033[33m\"\n",
        "BLUE = \"\\033[34m\"\n",
        "MAGENTA = \"\\033[35m\"\n",
        "CYAN = \"\\033[36m\"\n",
        "RESET = \"\\033[0m\"\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#learning rate backward propagation NN action\n",
        "lr_actor = 0.0003\n",
        "#learning rate backward propagation NN state value estimation\n",
        "lr_critic = 0.0003\n",
        "#Number of Learning Iteration we want to perform\n",
        "Iter = 100000\n",
        "#Number max of step to realise in one episode. \n",
        "MAX_STEP = 1000\n",
        "#How rewards are discounted.\n",
        "gamma =0.98\n",
        "#How do we stabilize variance in the return computation.\n",
        "lambd = 0.95\n",
        "#batch to train on\n",
        "batch_size = 64\n",
        "# Do we want high change to be taken into account.\n",
        "epsilon = 0.2\n",
        "#weight decay coefficient in ADAM for state value optim.\n",
        "l2_rate = 0.001\n",
        "\n",
        "\n",
        "# Actor class: Used to choose actions of a continuous action space.\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self,N_S,N_A):\n",
        "      # Initialize NN structure.\n",
        "        super(Actor,self).__init__()\n",
        "        self.fc1 = nn.Linear(N_S,64)\n",
        "        self.fc2 = nn.Linear(64,64)\n",
        "        self.sigma = nn.Linear(64,N_A)\n",
        "        self.mu = nn.Linear(64,N_A)\n",
        "        self.mu.weight.data.mul_(0.1)\n",
        "        self.mu.bias.data.mul_(0.0)\n",
        "        # This approach use gaussian distribution to decide actions. Could be\n",
        "        # something else.\n",
        "        self.distribution = torch.distributions.Normal\n",
        "\n",
        "    def set_init(self,layers):\n",
        "      # Initialize weight and bias according to a normal distrib mean 0 and sd 0.1.\n",
        "        for layer in layers:\n",
        "            nn.init.normal_(layer.weight,mean=0.,std=0.1)\n",
        "            nn.init.constant_(layer.bias,0.)\n",
        "\n",
        "    def forward(self,s):\n",
        "      # Use of tanh activation function is recommanded : bounded [-1,1],\n",
        "      # gives some non-linearity, and tends to give some stability.\n",
        "        x = torch.tanh(self.fc1(s))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        # mu action output of the NN.\n",
        "        mu = self.mu(x)\n",
        "        #log_sigma action output of the NN\n",
        "        log_sigma = self.sigma(x)\n",
        "        sigma = torch.exp(log_sigma)\n",
        "        return mu,sigma\n",
        "\n",
        "    def choose_action(self,s):\n",
        "      # Choose action in the continuous action space using normal distribution\n",
        "      # defined by mu and sigma of each actions returned by the NN.\n",
        "        mu,sigma = self.forward(s)\n",
        "        Pi = self.distribution(mu,sigma)\n",
        "        return Pi.sample().numpy()\n",
        "\n",
        "\n",
        "# Critic class : Used to estimate V(state) the state value function through a NN.\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self,N_S):\n",
        "      # Initialize NN structure.\n",
        "        super(Critic,self).__init__()\n",
        "        self.fc1 = nn.Linear(N_S,64)\n",
        "        self.fc2 = nn.Linear(64,64)\n",
        "        self.fc3 = nn.Linear(64,1)\n",
        "        self.fc3.weight.data.mul_(0.1)\n",
        "        self.fc3.bias.data.mul_(0.0)\n",
        "\n",
        "    def set_init(self,layers):\n",
        "      # Initialize weight and bias according to a normal distrib mean 0 and sd 0.1.\n",
        "        for layer in layers:\n",
        "            nn.init.normal_(layer.weight,mean=0.,std=0.1)\n",
        "            nn.init.constant_(layer.bias,0.)\n",
        "\n",
        "    def forward(self,s):\n",
        "      # Use of tanh activation function is recommanded.\n",
        "        x = torch.tanh(self.fc1(s))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        values = self.fc3(x)\n",
        "        return values\n",
        "\n",
        "\n",
        "class PPO:\n",
        "    def __init__(self,N_S,N_A):\n",
        "      # Initialize all the object we need for PPO.\n",
        "        self.actor_net =Actor(N_S,N_A)\n",
        "        self.critic_net = Critic(N_S)\n",
        "        self.actor_optim = optim.Adam(self.actor_net.parameters(),lr=lr_actor)\n",
        "        self.critic_optim = optim.Adam(self.critic_net.parameters(),lr=lr_critic,weight_decay=l2_rate)\n",
        "        self.critic_loss_func = torch.nn.MSELoss()\n",
        "\n",
        "    def train(self,memory):\n",
        "\n",
        "        #Easier to hande as np array and to separate everything for each episodes.\n",
        "        # \uac01 \uc694\uc18c\uac00 \ub3d9\uc77c\ud55c \ud615\ud0dc\ub97c \uac00\uc9c0\ub3c4\ub85d np.array\ub85c \ubcc0\ud658\ud558\uae30 \uc804\uc5d0 \ub9ac\uc2a4\ud2b8\ub85c \ucc98\ub9ac\n",
        "        states, actions, rewards, masks = [], [], [], []\n",
        "        \n",
        "        for m in memory:\n",
        "            states.append(m[0])\n",
        "            actions.append(m[1])\n",
        "            rewards.append(m[2])\n",
        "            masks.append(m[3])\n",
        "        \n",
        "        states = torch.tensor(np.array(states), dtype=torch.float32)\n",
        "        actions = torch.tensor(np.array(actions), dtype=torch.float32)\n",
        "        rewards = torch.tensor(np.array(rewards), dtype=torch.float32)\n",
        "        masks = torch.tensor(np.array(masks), dtype=torch.float32)\n",
        "\n",
        "        # Use critic network defined in Model.py\n",
        "        # This function enables to get the current state value V(S).\n",
        "        values = self.critic_net(states)\n",
        "        # Get advantage.\n",
        "        returns,advants = self.get_gae(rewards,masks,values)\n",
        "        #Get old mu and std.\n",
        "        old_mu,old_std = self.actor_net(states)\n",
        "        #Get the old distribution.\n",
        "        pi = self.actor_net.distribution(old_mu,old_std)\n",
        "        #Compute old policy.\n",
        "        old_log_prob = pi.log_prob(actions).sum(1,keepdim=True)\n",
        "\n",
        "        # Everything happens here\n",
        "        n = len(states)\n",
        "        arr = np.arange(n)\n",
        "        for epoch in range(1):\n",
        "            np.random.shuffle(arr)\n",
        "            for i in range(n//batch_size):\n",
        "                b_index = arr[batch_size*i:batch_size*(i+1)]\n",
        "                b_states = states[b_index]\n",
        "                b_advants = advants[b_index].unsqueeze(1)\n",
        "                b_actions = actions[b_index]\n",
        "                b_returns = returns[b_index].unsqueeze(1)\n",
        "\n",
        "                #New parameter of the policy distribution by action.\n",
        "                mu,std = self.actor_net(b_states)\n",
        "                pi = self.actor_net.distribution(mu,std)\n",
        "                new_prob = pi.log_prob(b_actions).sum(1,keepdim=True)\n",
        "                old_prob = old_log_prob[b_index].detach()\n",
        "                #Regularisation fixed KL : does not work as good as following clipping strategy\n",
        "                # empirically.\n",
        "                # KL_penalty = self.kl_divergence(old_mu[b_index],old_std[b_index],mu,std)\n",
        "                ratio = torch.exp(new_prob-old_prob)\n",
        "\n",
        "                surrogate_loss = ratio*b_advants\n",
        "                values = self.critic_net(b_states)\n",
        "                # MSE Loss : (State action value - State value)^2\n",
        "                critic_loss = self.critic_loss_func(values,b_returns)\n",
        "                # critic_loss = critic_loss - beta*KL_penalty\n",
        "\n",
        "                self.critic_optim.zero_grad()\n",
        "                critic_loss.backward()\n",
        "                self.critic_optim.step()\n",
        "                #Clipping strategy\n",
        "                ratio = torch.clamp(ratio,1.0-epsilon,1.0+epsilon)\n",
        "                clipped_loss =ratio*b_advants\n",
        "                # Actual loss\n",
        "                actor_loss = -torch.min(surrogate_loss,clipped_loss).mean()\n",
        "                #Now that we have the loss, we can do the backward propagation to learn : everything is here.\n",
        "                self.actor_optim.zero_grad()\n",
        "                actor_loss.backward()\n",
        "\n",
        "                self.actor_optim.step()\n",
        "                \n",
        "    # Get the Kullback - Leibler divergence: Measure of the diff btwn new and old policy:\n",
        "    # Could be used for the objective function depending on the strategy that needs to be\n",
        "    # teste.\n",
        "    def kl_divergence(self,old_mu,old_sigma,mu,sigma):\n",
        "\n",
        "        old_mu = old_mu.detach()\n",
        "        old_sigma = old_sigma.detach()\n",
        "\n",
        "        kl = torch.log(old_sigma) - torch.log(sigma) + (old_sigma.pow(2) + (old_mu - mu).pow(2)) / \\\n",
        "             (2.0 * sigma.pow(2)) - 0.5\n",
        "        return kl.sum(1, keepdim=True)\n",
        "    \n",
        "    # Advantage estimation:\n",
        "    def get_gae(self,rewards, masks, values):\n",
        "        rewards = torch.Tensor(rewards)\n",
        "        masks = torch.Tensor(masks)\n",
        "        #Create an equivalent fullfilled of 0.\n",
        "        returns = torch.zeros_like(rewards)\n",
        "        advants = torch.zeros_like(rewards)\n",
        "        #Init\n",
        "        running_returns = 0\n",
        "        previous_value = 0\n",
        "        running_advants = 0\n",
        "        #Here we compute A_t the advantage.\n",
        "        for t in reversed(range(0, len(rewards))):\n",
        "            # Here we compute the discounted returns. Gamma is the discount factor.\n",
        "            running_returns = rewards[t] + gamma * running_returns * masks[t]\n",
        "            #computes the difference between the estimated value at time step t (values.data[t]) and the discounted next value.\n",
        "            running_tderror = rewards[t] + gamma * previous_value * masks[t] - values.data[t]\n",
        "            # Compute advantage\n",
        "            running_advants = running_tderror + gamma * lambd * running_advants * masks[t]\n",
        "\n",
        "            returns[t] = running_returns\n",
        "            previous_value = values.data[t]\n",
        "            advants[t] = running_advants\n",
        "        #Normalization to stabilize final advantage of the history to now.\n",
        "        advants = (advants - advants.mean()) / advants.std()\n",
        "        return returns, advants\n",
        "\n",
        "# Creation of a class to normalize the states\n",
        "class Normalize:\n",
        "    def __init__(self, N_S):\n",
        "        self.mean = np.zeros((N_S,))\n",
        "        self.std = np.zeros((N_S, ))\n",
        "        self.stdd = np.zeros((N_S, ))\n",
        "        self.n = 0\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = np.asarray(x)\n",
        "        self.n += 1\n",
        "        if self.n == 1:\n",
        "            self.mean = x\n",
        "        else:\n",
        "            old_mean = self.mean.copy()\n",
        "            self.mean = old_mean + (x - old_mean) / self.n\n",
        "            self.stdd = self.stdd + (x - old_mean) * (x - self.mean)\n",
        "        if self.n > 1:\n",
        "            self.std = np.sqrt(self.stdd / (self.n - 1))\n",
        "        else:\n",
        "            self.std = self.mean\n",
        "\n",
        "        x = x - self.mean\n",
        "        x = x / (self.std + 1e-8)\n",
        "        x = np.clip(x, -5, +5)\n",
        "        return x\n",
        "\n",
        "\n",
        "env = gym.make('Walker2d-v4', render_mode='human')\n",
        "\n",
        "#Number of state and action\n",
        "N_S = env.observation_space.shape[0]\n",
        "N_A = env.action_space.shape[0]\n",
        "\n",
        "# Random seed initialization\n",
        "# env.seed(500)\n",
        "# torch.manual_seed(500)\n",
        "# np.random.seed(500)\n",
        "\n",
        "# Run the Ppo class\n",
        "frames = []\n",
        "ppo = PPO(N_S,N_A)\n",
        "# Normalisation for stability, fast convergence... always good to do.\n",
        "normalize = Normalize(N_S)\n",
        "episodes = 0\n",
        "eva_episodes = 0\n",
        "state, _ = env.reset()\n",
        "\n",
        "for iter in tqdm(range(Iter)):\n",
        "    memory = deque()\n",
        "    scores = []\n",
        "    steps = 0\n",
        "    while steps < 2048: #Horizon\n",
        "        episodes += 1\n",
        "        state, _ = env.reset()\n",
        "        s = normalize(state)\n",
        "        score = 0\n",
        "        for _ in range(MAX_STEP):\n",
        "            steps += 1\n",
        "            #Choose an action: detailed in PPO.py\n",
        "            # The action is a numpy array of 17 elements. It means that in the 17 possible directions of action we have a specific value in the continuous space.\n",
        "            # Exemple : the first coordinate correspond to the Torque applied on the hinge in the y-coordinate of the abdomen: this is continuous space.\n",
        "            a=ppo.actor_net.choose_action(torch.from_numpy(np.array(s).astype(np.float32)).unsqueeze(0))[0]\n",
        "\n",
        "            #Environnement reaction to the action : There is a reaction in the 376 elements that characterize the space :\n",
        "            # Exemple : the first coordinate of the states is the z-coordinate of the torso (centre) and using env.step(a), we get the reaction of this state and\n",
        "            # of all the other ones after the action has been made.\n",
        "            s_ , r ,truncated, terminated ,info = env.step(a)\n",
        "            done = truncated or terminated\n",
        "            s_ = normalize(s_)\n",
        "\n",
        "            # Do we continue or do we terminate an episode?\n",
        "            mask = (1-done)*1\n",
        "            memory.append([s,a,r,mask])\n",
        "            # print('s: ', s)\n",
        "            # print('a: ', a)\n",
        "            # print('r: ', r)\n",
        "            # print('mask: ', mask)\n",
        "            \n",
        "            score += r\n",
        "            s = s_\n",
        "            if done:\n",
        "                break\n",
        "        # with open('log_' + args.env_name  + '.txt', 'a') as outfile:\n",
        "        #     outfile.write('\\t' + str(episodes)  + '\\t' + str(score) + '\\n')\n",
        "        scores.append(score)\n",
        "    score_avg = np.mean(scores)\n",
        "    print('{} episode score is {:.2f}'.format(episodes, score_avg))\n",
        "    ppo.train(memory)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Hyperparameters\n",
        "# learning_rate = 0.0003\n",
        "# gamma = 0.9\n",
        "# lmbda = 0.9\n",
        "# eps_clip = 0.2\n",
        "# K_epoch = 10\n",
        "# rollout_len = 3\n",
        "# buffer_size = 10\n",
        "# minibatch_size = 32\n",
        "\n",
        "# class PPO(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(PPO, self).__init__()\n",
        "#         self.data = []\n",
        "\n",
        "#         self.fc1 = nn.Linear(17, 128)\n",
        "#         self.fc_mu = nn.Linear(128, 6)\n",
        "#         self.fc_std = nn.Linear(128, 6)\n",
        "#         self.fc_v = nn.Linear(128, 1)\n",
        "#         self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "#         self.optimization_step = 0\n",
        "\n",
        "#     def pi(self, x, softmax_dim=0):\n",
        "#         x = F.relu(self.fc1(x))\n",
        "#         mu = 2.0 * torch.tanh(self.fc_mu(x))\n",
        "#         std = F.softplus(self.fc_std(x))\n",
        "#         return mu, std\n",
        "\n",
        "#     def v(self, x):\n",
        "#         x = F.relu(self.fc1(x))\n",
        "#         v = self.fc_v(x)\n",
        "#         return v\n",
        "\n",
        "#     def put_data(self, transition):\n",
        "#         print(f\"{YELLOW}transition: \\n{RESET}\", transition)\n",
        "#         self.data.append(transition)\n",
        "\n",
        "#     def make_batch(self):\n",
        "#         s_batch, a_batch, r_batch, s_prime_batch, prob_a_batch, done_batch = [], [], [], [], [], []\n",
        "#         data = []\n",
        "\n",
        "#         for j in range(buffer_size):\n",
        "#             s_lst, a_lst, r_lst, s_prime_lst, prob_a_lst, done_lst = [], [], [], [], [], []\n",
        "#             for i in range(minibatch_size):\n",
        "#                 rollout = self.data.pop()\n",
        "\n",
        "#                 for transition in rollout:\n",
        "#                     s, a, r, s_prime, prob_a, done = transition\n",
        "\n",
        "#                     s_lst.append(s)\n",
        "#                     a_lst.append(a)\n",
        "#                     r_lst.append(r)\n",
        "#                     s_prime_lst.append(s_prime)\n",
        "#                     prob_a_lst.append(prob_a)\n",
        "#                     done_mask = 0.0 if done else 1.0\n",
        "#                     done_lst.append(done_mask)\n",
        "\n",
        "#                 s_batch.append(s_lst)\n",
        "#                 a_batch.append(a_lst)\n",
        "#                 r_batch.append(r_lst)\n",
        "#                 s_prime_batch.append(s_prime_lst)\n",
        "#                 prob_a_batch.append(prob_a_lst)\n",
        "#                 done_batch.append(done_lst)\n",
        "\n",
        "#             s_batch_tensor = torch.tensor(s_batch, dtype=torch.float).view(-1, 17)\n",
        "#             a_batch_tensor = torch.tensor(a_batch, dtype=torch.float).view(-1, 6)\n",
        "#             r_batch_tensor = torch.tensor(r_batch, dtype=torch.float).view(-1, 1)\n",
        "#             s_prime_batch_tensor = torch.tensor(s_prime_batch, dtype=torch.float).view(-1, 17)\n",
        "#             prob_a_batch_tensor = torch.tensor(prob_a_batch, dtype=torch.float).view(-1, 6)\n",
        "#             done_batch_tensor = torch.tensor(done_batch, dtype=torch.float).view(-1, 1)\n",
        "\n",
        "#             mini_batch = (s_batch_tensor, a_batch_tensor, r_batch_tensor,\n",
        "#                           s_prime_batch_tensor, done_batch_tensor, prob_a_batch_tensor)\n",
        "\n",
        "#             data.append(mini_batch)\n",
        "\n",
        "#         return data\n",
        "\n",
        "#     def calc_advantage(self, data):\n",
        "#         data_with_adv = []\n",
        "#         for mini_batch in data:\n",
        "#             s, a, r, s_prime, done_mask, old_log_prob = mini_batch\n",
        "#             with torch.no_grad():\n",
        "#                 td_target = r + gamma * self.v(s_prime).squeeze(-1) * done_mask\n",
        "#                 delta = td_target - self.v(s).squeeze(-1)\n",
        "#             delta = delta.numpy()\n",
        "\n",
        "#             advantage_lst = []\n",
        "#             advantage = 0.0\n",
        "#             for delta_t in delta[::-1]:\n",
        "#                 advantage = gamma * lmbda * advantage + delta_t[0]\n",
        "#                 advantage_lst.append([advantage])\n",
        "#             advantage_lst.reverse()\n",
        "#             advantage = torch.tensor(advantage_lst, dtype=torch.float)\n",
        "#             advantage = advantage.view(-1, 1)  # \uc5ec\uae30\uc5d0 \ucc28\uc6d0 \ub9de\ucd94\uae30 \ucd94\uac00\n",
        "#             data_with_adv.append((s, a, r, s_prime, done_mask, old_log_prob, td_target, advantage))\n",
        "\n",
        "#         return data_with_adv\n",
        "\n",
        "#     def train_net(self):\n",
        "#         if len(self.data) == minibatch_size * buffer_size:\n",
        "#             data = self.make_batch()\n",
        "#             data = self.calc_advantage(data)\n",
        "\n",
        "#             for i in range(K_epoch):\n",
        "#                 for mini_batch in data:\n",
        "#                     s, a, r, s_prime, done_mask, old_log_prob, td_target, advantage = mini_batch\n",
        "\n",
        "#                     mu, std = self.pi(s, softmax_dim=1)\n",
        "#                     dist = Normal(mu, std)\n",
        "#                     log_prob = dist.log_prob(a)\n",
        "#                     ratio = torch.exp(log_prob.sum(dim=1, keepdim=True) - old_log_prob.sum(dim=1, keepdim=True))  # \ucc28\uc6d0 \ub9de\ucd94\uae30 \ucd94\uac00\n",
        "\n",
        "#                     surr1 = ratio * advantage\n",
        "#                     surr2 = torch.clamp(ratio, 1 - eps_clip, 1 + eps_clip) * advantage\n",
        "#                     loss = -torch.min(surr1, surr2) + F.smooth_l1_loss(self.v(s).squeeze(-1), td_target)\n",
        "\n",
        "#                     self.optimizer.zero_grad()\n",
        "#                     loss.mean().backward()\n",
        "#                     nn.utils.clip_grad_norm_(self.parameters(), 1.0)\n",
        "#                     self.optimizer.step()\n",
        "#                     self.optimization_step += 1\n",
        "\n",
        "# def main():\n",
        "#     env = gym.make('Walker2d-v4', render_mode='human')\n",
        "#     model = PPO()\n",
        "#     score = 0.0\n",
        "#     print_interval = 20\n",
        "#     rollout = []\n",
        "\n",
        "#     for n_epi in tqdm(range(10000)):\n",
        "#         s, _ = env.reset()\n",
        "#         done = False\n",
        "#         count = 0\n",
        "#         while count < 200 and not done:\n",
        "#             for t in range(rollout_len):\n",
        "#                 mu, std = model.pi(torch.from_numpy(s).float())\n",
        "#                 dist = Normal(mu, std)\n",
        "#                 a = dist.sample()\n",
        "#                 log_prob = dist.log_prob(a)\n",
        "\n",
        "#                 a = a.cpu().detach().numpy()\n",
        "#                 print(f\"{RED}log_prob: {RESET}\\n\", log_prob)\n",
        "\n",
        "#                 s_prime, r, done, truncated, info = env.step(a)\n",
        "\n",
        "#                 rollout.append((s, a, r / 10.0, s_prime, log_prob, done))\n",
        "#                 if len(rollout) == rollout_len:\n",
        "#                     model.put_data(rollout)\n",
        "#                     rollout = []\n",
        "\n",
        "#                 s = s_prime\n",
        "#                 score += r\n",
        "#                 count += 1\n",
        "\n",
        "#             model.train_net()\n",
        "\n",
        "#         if n_epi % print_interval == 0 and n_epi != 0:\n",
        "#             print(\"# of episode :{}, avg score : {:.1f}, optmization step: {}\".format(n_epi, score / print_interval, model.optimization_step))\n",
        "#             score = 0.0\n",
        "\n",
        "#     env.close()\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     main()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}