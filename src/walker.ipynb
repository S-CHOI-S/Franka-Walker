{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "\n",
    "import time\n",
    "import psutil\n",
    "import datetime\n",
    "import subprocess\n",
    "# import torch\n",
    "import torchvision\n",
    "from tensorboard import program\n",
    "import webbrowser\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from color_code import *\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#learning rate backward propagation NN action\n",
    "lr_actor = 0.0003\n",
    "#learning rate backward propagation NN state value estimation\n",
    "lr_critic = 0.0003\n",
    "#Number of Learning Iteration we want to perform\n",
    "Iter = 100000\n",
    "#Number max of step to realise in one episode. \n",
    "MAX_STEP = 1000\n",
    "#How rewards are discounted.\n",
    "gamma =0.98\n",
    "#How do we stabilize variance in the return computation.\n",
    "lambd = 0.95\n",
    "#batch to train on\n",
    "batch_size = 64\n",
    "# Do we want high change to be taken into account.\n",
    "epsilon = 0.2\n",
    "#weight decay coefficient in ADAM for state value optim.\n",
    "l2_rate = 0.001\n",
    "\n",
    "save_freq = 100\n",
    "\n",
    "save_flag = False\n",
    "\n",
    "cstrnt1_limit = 0.2 # y angle of the torso\n",
    "cstrnt2_limit = 0.5 # x vel of the torso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor class: Used to choose actions of a continuous action space.\n",
    "class Actor(nn.Module):\n",
    "  def __init__(self, N_S, N_A, chkpt_dir):\n",
    "    # Initialize NN structure.\n",
    "      super(Actor,self).__init__()\n",
    "      self.fc1 = nn.Linear(N_S,64)\n",
    "      self.fc2 = nn.Linear(64,64)\n",
    "      self.sigma = nn.Linear(64,N_A)\n",
    "      self.mu = nn.Linear(64,N_A)\n",
    "      self.mu.weight.data.mul_(0.1)\n",
    "      self.mu.bias.data.mul_(0.0)\n",
    "      # This approach use gaussian distribution to decide actions. Could be\n",
    "      # something else.\n",
    "      self.distribution = torch.distributions.Normal\n",
    "      \n",
    "      self.checkpoint_dir = chkpt_dir\n",
    "      self.checkpoint_file = os.path.join(self.checkpoint_dir, '_actor')\n",
    "      \n",
    "      self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "      self.to(self.device)\n",
    "\n",
    "  def set_init(self,layers):\n",
    "    # Initialize weight and bias according to a normal distrib mean 0 and sd 0.1.\n",
    "      for layer in layers:\n",
    "          nn.init.normal_(layer.weight,mean=0.,std=0.1)\n",
    "          nn.init.constant_(layer.bias,0.)\n",
    "\n",
    "  def forward(self,s):\n",
    "    # Use of tanh activation function is recommanded : bounded [-1,1],\n",
    "    # gives some non-linearity, and tends to give some stability.\n",
    "      x = torch.tanh(self.fc1(s))\n",
    "      x = torch.tanh(self.fc2(x))\n",
    "      # mu action output of the NN.\n",
    "      mu = self.mu(x)\n",
    "      #log_sigma action output of the NN\n",
    "      log_sigma = self.sigma(x)\n",
    "      sigma = torch.exp(log_sigma)\n",
    "      return mu,sigma\n",
    "\n",
    "  def choose_action(self,s):\n",
    "    # Choose action in the continuous action space using normal distribution\n",
    "    # defined by mu and sigma of each actions returned by the NN.\n",
    "      s = torch.from_numpy(np.array(s).astype(np.float32)).unsqueeze(0).to(self.device)\n",
    "      mu,sigma = self.forward(s)\n",
    "      Pi = self.distribution(mu,sigma)\n",
    "      return Pi.sample().cpu().numpy().squeeze(0)\n",
    "  \n",
    "  def save_model(self):\n",
    "      torch.save(self.state_dict(), self.checkpoint_file)\n",
    "      \n",
    "  def load_model(self, load_model_dir=None):\n",
    "      if load_model_dir is None:\n",
    "          load_model_dir = self.checkpoint_file\n",
    "      self.load_state_dict(torch.load(load_model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic class : Used to estimate V(state) the state value function through a NN.\n",
    "class Critic(nn.Module):\n",
    "  def __init__(self, N_S, chkpt_dir):\n",
    "    # Initialize NN structure.\n",
    "      super(Critic,self).__init__()\n",
    "      self.fc1 = nn.Linear(N_S,64)\n",
    "      self.fc2 = nn.Linear(64,64)\n",
    "      self.fc3 = nn.Linear(64,1)\n",
    "      self.fc3.weight.data.mul_(0.1) # 초기 weight에 0.1을 곱해주면서 학습을 더 안정적으로 할 수 있도록(tanh, sigmoid를 사용할 경우 많이 쓰는 방식)\n",
    "      self.fc3.bias.data.mul_(0.0) # bias tensor의 모든 원소를 0으로 설정\n",
    "      \n",
    "      self.checkpoint_dir = chkpt_dir\n",
    "      self.checkpoint_file = os.path.join(self.checkpoint_dir, '_critic')\n",
    "      \n",
    "      self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "      self.to(self.device)\n",
    "\n",
    "  def set_init(self,layers):\n",
    "    # Initialize weight and bias according to a normal distrib mean 0 and sd 0.1.\n",
    "      for layer in layers:\n",
    "          nn.init.normal_(layer.weight,mean=0.,std=0.1)\n",
    "          nn.init.constant_(layer.bias,0.)\n",
    "\n",
    "  def forward(self,s):\n",
    "    # Use of tanh activation function is recommanded.\n",
    "      x = torch.tanh(self.fc1(s))\n",
    "      x = torch.tanh(self.fc2(x))\n",
    "      values = self.fc3(x)\n",
    "      return values\n",
    "  \n",
    "  def save_model(self):\n",
    "      torch.save(self.state_dict(), self.checkpoint_file)\n",
    "      \n",
    "  def load_model(self, load_model_dir=None):\n",
    "      if load_model_dir is None:\n",
    "          load_model_dir = self.checkpoint_file\n",
    "      self.load_state_dict(torch.load(load_model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Algorithm with Constraints   \n",
    "class PPO:\n",
    "    def __init__(self, N_S, N_A, log_dir):\n",
    "        self.log_dir = log_dir\n",
    "        \n",
    "        self.actor_net = Actor(N_S, N_A, log_dir)\n",
    "        self.critic_net = Critic(N_S, log_dir)\n",
    "        self.actor_optim = optim.Adam(self.actor_net.parameters(), lr=1e-4)\n",
    "        self.critic_optim = optim.Adam(self.critic_net.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "        self.critic_loss_func = torch.nn.MSELoss()\n",
    "        \n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def train(self, memory):\n",
    "        states, actions, rewards, masks = [], [], [], []\n",
    "        \n",
    "        for m in memory:\n",
    "            states.append(m[0])\n",
    "            actions.append(m[1])\n",
    "            rewards.append(m[2])\n",
    "            masks.append(m[3])\n",
    "        \n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(np.array(actions), dtype=torch.float32).to(self.device)\n",
    "        rewards = torch.tensor(np.array(rewards), dtype=torch.float32).to(self.device)\n",
    "        masks = torch.tensor(np.array(masks), dtype=torch.float32).to(self.device)\n",
    "\n",
    "        # Use critic network defined in Model.py\n",
    "        # This function enables to get the current state value V(S).\n",
    "        values = self.critic_net(states)\n",
    "        # Get advantage.\n",
    "        returns,advants = self.get_gae(rewards,masks,values)\n",
    "        #Get old mu and std.\n",
    "        old_mu,old_std = self.actor_net(states)\n",
    "        #Get the old distribution.\n",
    "        pi = self.actor_net.distribution(old_mu,old_std)\n",
    "        #Compute old policy.\n",
    "        old_log_prob = pi.log_prob(actions).sum(1,keepdim=True)\n",
    "\n",
    "        # Everything happens here\n",
    "        n = len(states)\n",
    "        arr = np.arange(n)\n",
    "        for epoch in range(1):\n",
    "            np.random.shuffle(arr)\n",
    "            for i in range(n//batch_size):\n",
    "                b_index = arr[batch_size*i:batch_size*(i+1)]\n",
    "                b_states = states[b_index]\n",
    "                b_advants = advants[b_index].unsqueeze(1)\n",
    "                b_actions = actions[b_index]\n",
    "                b_returns = returns[b_index].unsqueeze(1)\n",
    "\n",
    "                #New parameter of the policy distribution by action.\n",
    "                mu,std = self.actor_net(b_states)\n",
    "                pi = self.actor_net.distribution(mu,std)\n",
    "                new_prob = pi.log_prob(b_actions).sum(1,keepdim=True)\n",
    "                old_prob = old_log_prob[b_index].detach()\n",
    "                #Regularisation fixed KL : does not work as good as following clipping strategy\n",
    "                # empirically.\n",
    "                # KL_penalty = self.kl_divergence(old_mu[b_index],old_std[b_index],mu,std)\n",
    "                ratio = torch.exp(new_prob-old_prob)\n",
    "\n",
    "                surrogate_loss = ratio*b_advants\n",
    "                values = self.critic_net(b_states)\n",
    "                # MSE Loss : (State action value - State value)^2\n",
    "                critic_loss = self.critic_loss_func(values,b_returns)\n",
    "                # critic_loss = critic_loss - beta*KL_penalty\n",
    "\n",
    "                self.critic_optim.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                self.critic_optim.step()\n",
    "                #Clipping strategy\n",
    "                ratio = torch.clamp(ratio,1.0-epsilon,1.0+epsilon)\n",
    "                clipped_loss =ratio*b_advants\n",
    "                # Actual loss\n",
    "                actor_loss = -torch.min(surrogate_loss,clipped_loss).mean()\n",
    "                \n",
    "                walker_cstrnt1 = torch.tensor([get_walker_constraints(state, 1) for state in b_states], dtype=torch.float32).to(self.device)\n",
    "                actor_loss = augmented_objective(actor_loss, walker_cstrnt1, cstrnt1_limit, 20)\n",
    "                \n",
    "                walker_cstrnt2 = torch.tensor([get_walker_constraints(state, 8) for state in b_states], dtype=torch.float32).to(self.device)\n",
    "                actor_loss = augmented_objective(actor_loss, walker_cstrnt2, cstrnt2_limit, 20)\n",
    "                \n",
    "                #Now that we have the loss, we can do the backward propagation to learn : everything is here.\n",
    "                self.actor_optim.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_optim.step()\n",
    "                \n",
    "    # Get the Kullback - Leibler divergence: Measure of the diff btwn new and old policy:\n",
    "    # Could be used for the objective function depending on the strategy that needs to be\n",
    "    # teste.\n",
    "    def kl_divergence(self,old_mu,old_sigma,mu,sigma):\n",
    "\n",
    "        old_mu = old_mu.detach()\n",
    "        old_sigma = old_sigma.detach()\n",
    "\n",
    "        kl = torch.log(old_sigma) - torch.log(sigma) + (old_sigma.pow(2) + (old_mu - mu).pow(2)) / \\\n",
    "                (2.0 * sigma.pow(2)) - 0.5\n",
    "        return kl.sum(1, keepdim=True)\n",
    "\n",
    "    # Advantage estimation:\n",
    "    def get_gae(self,rewards, masks, values):\n",
    "        rewards = torch.Tensor(rewards).to(self.device)\n",
    "        masks = torch.Tensor(masks).to(self.device)\n",
    "        #Create an equivalent fullfilled of 0.\n",
    "        returns = torch.zeros_like(rewards).to(self.device)\n",
    "        advants = torch.zeros_like(rewards).to(self.device)\n",
    "        #Init\n",
    "        running_returns = 0\n",
    "        previous_value = 0\n",
    "        running_advants = 0\n",
    "        #Here we compute A_t the advantage.\n",
    "        for t in reversed(range(0, len(rewards))):\n",
    "            # Here we compute the discounted returns. Gamma is the discount factor.\n",
    "            running_returns = rewards[t] + gamma * running_returns * masks[t]\n",
    "            #computes the difference between the estimated value at time step t (values.data[t]) and the discounted next value.\n",
    "            running_tderror = rewards[t] + gamma * previous_value * masks[t] - values.data[t]\n",
    "            # Compute advantage\n",
    "            running_advants = running_tderror + gamma * lambd * running_advants * masks[t]\n",
    "\n",
    "            returns[t] = running_returns\n",
    "            previous_value = values.data[t]\n",
    "            advants[t] = running_advants\n",
    "        #Normalization to stabilize final advantage of the history to now.\n",
    "        advants = (advants - advants.mean()) / advants.std()\n",
    "        return returns, advants\n",
    "\n",
    "    def save(self):\n",
    "        # filename = str(filename)\n",
    "        torch.save(self.actor_optim.state_dict(), self.log_dir + \"_actor_optimizer\")\n",
    "        torch.save(self.critic_optim.state_dict(), self.log_dir + \"_critic_optimizer\")\n",
    "\n",
    "    def load(self, log_dir=None):\n",
    "        # filename = str(filename)\n",
    "        if log_dir == None:\n",
    "            log_dir = self.log_dir\n",
    "        self.actor_optim.load_state_dict(torch.load(log_dir + \"_actor_optimizer\"))\n",
    "        self.critic_optim.load_state_dict(torch.load(log_dir + \"_critic_optimizer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of a class to normalize the states\n",
    "class Normalize:\n",
    "    def __init__(self, N_S, chkpt_dir, train_mode=True):\n",
    "        self.mean = np.zeros((N_S,))\n",
    "        self.std = np.zeros((N_S, ))\n",
    "        self.stdd = np.zeros((N_S, ))\n",
    "        self.n = 0\n",
    "        \n",
    "        self.train_mode = train_mode\n",
    "        \n",
    "        self.checkpoint_dir = chkpt_dir\n",
    "        self.checkpoint_file = os.path.join(self.checkpoint_dir, '_normalize.npy')\n",
    "        \n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = np.asarray(x)\n",
    "        if self.train_mode:\n",
    "            self.n += 1\n",
    "            if self.n == 1:\n",
    "                self.mean = x\n",
    "            else:\n",
    "                old_mean = self.mean.copy()\n",
    "                self.mean = old_mean + (x - old_mean) / self.n\n",
    "                self.stdd = self.stdd + (x - old_mean) * (x - self.mean)\n",
    "            if self.n > 1:\n",
    "                self.std = np.sqrt(self.stdd / (self.n - 1))\n",
    "            else:\n",
    "                self.std = self.mean\n",
    "\n",
    "        x = x - self.mean\n",
    "        x = x / (self.std + 1e-8)\n",
    "        x = np.clip(x, -5, +5)\n",
    "        return x\n",
    "    \n",
    "    def update(self, x):\n",
    "        self.mean = np.mean(x, axis=0)\n",
    "        self.std = np.std(x, axis=0) + 1e-8\n",
    "    \n",
    "    def save_params(self):\n",
    "        np.save(self.checkpoint_file, {'mean': self.mean, 'std': self.std})\n",
    "\n",
    "    def load_params(self, load_model_dir=None):\n",
    "        if load_model_dir is None:\n",
    "            load_model_dir = self.checkpoint_file\n",
    "        params = np.load(load_model_dir, allow_pickle=True).item()\n",
    "        self.mean = params['mean']\n",
    "        self.std = params['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_walker_constraints(state, num):\n",
    "    x_vel = state[num]\n",
    "    return x_vel\n",
    "\n",
    "def logarithmic_barrier(state, constraint_max):\n",
    "    indicator = torch.where((state - constraint_max) <= 0, torch.tensor((state - constraint_max), device=state.device), torch.tensor(0, device=state.device))\n",
    "    return -torch.log(-indicator)\n",
    "\n",
    "def augmented_objective(actor_loss, state, constraint_max, t):\n",
    "    constraint_barrier = logarithmic_barrier(state, constraint_max) / t\n",
    "    return actor_loss + constraint_barrier.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = gym.make('Walker2d-v4', render_mode='human')\n",
    "\n",
    "    #Number of state and action\n",
    "    N_S = env.observation_space.shape[0]\n",
    "    N_A = env.action_space.shape[0]\n",
    "\n",
    "    # Run the Ppo class\n",
    "    frames = []\n",
    "    ppo = PPO(N_S, N_A, log_dir)\n",
    "    \n",
    "    # Normalization for stability, fast convergence... always good to do.\n",
    "    normalize = Normalize(N_S, log_dir)\n",
    "    \n",
    "    # ppo.actor_net.load_model('../runs/20240715_19-42-33/_actor')\n",
    "    # ppo.actor_net.eval()\n",
    "    # ppo.critic_net.load_model('../runs/20240715_19-42-33/_critic')\n",
    "    # ppo.critic_net.eval()\n",
    "    # ppo.load('../runs/20240715_19-42-33/')\n",
    "    # normalize.load_params('../runs/20240715_19-42-33/_normalize.npy')\n",
    "\n",
    "    episodes = 0\n",
    "    eva_episodes = 0\n",
    "    episode_data = []\n",
    "    constraint_data = []\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    for iter in tqdm(range(Iter)):\n",
    "        memory = deque()\n",
    "        scores = []\n",
    "        steps = 0\n",
    "        cstrnt1 = []\n",
    "        cstrnt2 = []\n",
    "        while steps < 2048: #Horizon\n",
    "            episodes += 1\n",
    "            state, _ = env.reset()\n",
    "            s = normalize(state)\n",
    "            score = 0\n",
    "            for _ in range(MAX_STEP):\n",
    "                steps += 1\n",
    "                #Choose an action: detailed in PPO.py\n",
    "                # The action is a numpy array of 17 elements. It means that in the 17 possible directions of action we have a specific value in the continuous space.\n",
    "                # Exemple : the first coordinate correspond to the Torque applied on the hinge in the y-coordinate of the abdomen: this is continuous space.\n",
    "                a = ppo.actor_net.choose_action(s)\n",
    "                # print(f\"{YELLOW}walker velocity: {RESET}\", s[8]) # 3\n",
    "                #Environnement reaction to the action : There is a reaction in the 376 elements that characterize the space :\n",
    "                # Example : the first coordinate of the states is the z-coordinate of the torso (centre) and using env.step(a), we get the reaction of this state and\n",
    "                # of all the other ones after the action has been made.\n",
    "                s_ , r ,truncated, terminated ,info = env.step(a)\n",
    "                s_ = normalize(s_)\n",
    "                done = truncated or terminated\n",
    "\n",
    "                # Do we continue or do we terminate an episode?\n",
    "                mask = (1-done)*1\n",
    "                memory.append([s,a,r,mask])\n",
    "                cstrnt1.append(s[1])\n",
    "                cstrnt2.append(s[8])\n",
    "                score += r\n",
    "                s = s_\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "            # with open('log_' + args.env_name  + '.txt', 'a') as outfile:\n",
    "            #     outfile.write('\\t' + str(episodes)  + '\\t' + str(score) + '\\n')\n",
    "            scores.append(score)\n",
    "        \n",
    "        score_avg = np.mean(scores)\n",
    "        cstrnt1_avg = np.mean(cstrnt1)\n",
    "        cstrnt2_avg = np.mean(cstrnt2)\n",
    "        \n",
    "        if (cstrnt1_avg <= cstrnt1_limit) & (cstrnt2_avg <= cstrnt2_limit):\n",
    "            print(f\"{episodes} episode score is {score_avg:.2f}, y_angle_of_the_torso is {GREEN}{cstrnt1_avg:.3f}{RESET}, x_vel_of_the_torso is {GREEN}{cstrnt2_avg:.3f}{RESET}\")\n",
    "        elif (cstrnt1_avg <= cstrnt1_limit) & (cstrnt2_avg > cstrnt2_limit):\n",
    "            print(f\"{episodes} episode score is {score_avg:.2f}, y_angle_of_the_torso is {GREEN}{cstrnt1_avg:.3f}{RESET}, x_vel_of_the_torso is {RED}{cstrnt2_avg:.3f}{RESET}\")\n",
    "        elif (cstrnt1_avg > cstrnt1_limit) & (cstrnt2_avg <= cstrnt2_limit):\n",
    "            print(f\"{episodes} episode score is {score_avg:.2f}, y_angle_of_the_torso is {RED}{cstrnt1_avg:.3f}{RESET}, x_vel_of_the_torso is {GREEN}{cstrnt2_avg:.3f}{RESET}\")\n",
    "        else:\n",
    "            print(f\"{episodes} episode score is {score_avg:.2f}, y_angle_of_the_torso is {RED}{cstrnt1_avg:.3f}{RESET}, x_vel_of_the_torso is {RED}{cstrnt2_avg:.3f}{RESET}\")\n",
    "        \n",
    "        episode_data.append([iter + 1, score_avg])\n",
    "        constraint_data.append([iter + 1, cstrnt1_limit, cstrnt1_avg, cstrnt2_limit, cstrnt2_avg])\n",
    "        \n",
    "        if (iter + 1) % save_freq == 0:\n",
    "            save_flag = True\n",
    "\n",
    "            if save_flag:\n",
    "                ppo.actor_net.save_model()\n",
    "                ppo.critic_net.save_model()\n",
    "                ppo.save()\n",
    "                normalize.save_params()\n",
    "                print(f\"{GREEN} >> Successfully saved models! {RESET}\")\n",
    "\n",
    "                np.save(log_dir + \"reward.npy\", episode_data)\n",
    "                np.save(log_dir + \"constraint.npy\", constraint_data)\n",
    "                print(f\"{GREEN} >> Successfully saved reward & constraint data! {RESET}\")\n",
    "                \n",
    "                save_flag = False\n",
    "\n",
    "        ppo.train(memory)\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H-%M-%S\")\n",
    "    log_dir = f\"../runs/{current_time}/\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    writer = SummaryWriter(log_dir)\n",
    "    print(f\"{YELLOW}[MODEL/TENSORBOARD]{RESET} The data will be saved in {YELLOW}{log_dir}{RESET} directory!\")\n",
    "\n",
    "    # tb = program.TensorBoard()\n",
    "    # tb.configure(argv=[None, '--logdir', f\"../runs/franka_cabinet/{current_time}\", '--port', '6300'])\n",
    "    # url = tb.launch()\n",
    "    # webbrowser.open_new(url)\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('average score: ', score_avg)\n",
    "print('average xvel:  ', xvel_avg)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
