{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[MODEL/TENSORBOARD]\u001b[0m The data will be saved in \u001b[33m../runs/20240712_02-38-11/\u001b[0m directory!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "\n",
    "import time\n",
    "import psutil\n",
    "import datetime\n",
    "import subprocess\n",
    "# import torch\n",
    "import torchvision\n",
    "from tensorboard import program\n",
    "import webbrowser\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "RED = \"\\033[31m\"\n",
    "GREEN = \"\\033[32m\"\n",
    "YELLOW = \"\\033[33m\"\n",
    "BLUE = \"\\033[34m\"\n",
    "MAGENTA = \"\\033[35m\"\n",
    "CYAN = \"\\033[36m\"\n",
    "RESET = \"\\033[0m\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H-%M-%S\")\n",
    "log_dir = f\"../runs/{current_time}/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "writer = SummaryWriter(log_dir)\n",
    "print(f\"{YELLOW}[MODEL/TENSORBOARD]{RESET} The data will be saved in {YELLOW}{log_dir}{RESET} directory!\")\n",
    "\n",
    "# tb = program.TensorBoard()\n",
    "# tb.configure(argv=[None, '--logdir', f\"../runs/franka_cabinet/{current_time}\", '--port', '6300'])\n",
    "# url = tb.launch()\n",
    "# webbrowser.open_new(url)\n",
    "\n",
    "#learning rate backward propagation NN action\n",
    "lr_actor = 0.0003\n",
    "#learning rate backward propagation NN state value estimation\n",
    "lr_critic = 0.0003\n",
    "#Number of Learning Iteration we want to perform\n",
    "Iter = 100000\n",
    "#Number max of step to realise in one episode. \n",
    "MAX_STEP = 1000\n",
    "#How rewards are discounted.\n",
    "gamma =0.98\n",
    "#How do we stabilize variance in the return computation.\n",
    "lambd = 0.95\n",
    "#batch to train on\n",
    "batch_size = 64\n",
    "# Do we want high change to be taken into account.\n",
    "epsilon = 0.2\n",
    "#weight decay coefficient in ADAM for state value optim.\n",
    "l2_rate = 0.001\n",
    "\n",
    "save_freq = 100\n",
    "\n",
    "save_flag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor class: Used to choose actions of a continuous action space.\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, N_S, N_A, chkpt_dir):\n",
    "      # Initialize NN structure.\n",
    "        super(Actor,self).__init__()\n",
    "        self.fc1 = nn.Linear(N_S,64)\n",
    "        self.fc2 = nn.Linear(64,64)\n",
    "        self.sigma = nn.Linear(64,N_A)\n",
    "        self.mu = nn.Linear(64,N_A)\n",
    "        self.mu.weight.data.mul_(0.1)\n",
    "        self.mu.bias.data.mul_(0.0)\n",
    "        # This approach use gaussian distribution to decide actions. Could be\n",
    "        # something else.\n",
    "        self.distribution = torch.distributions.Normal\n",
    "        \n",
    "        self.checkpoint_dir = chkpt_dir\n",
    "        self.checkpoint_file = os.path.join(self.checkpoint_dir, '_actor')\n",
    "        \n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def set_init(self,layers):\n",
    "      # Initialize weight and bias according to a normal distrib mean 0 and sd 0.1.\n",
    "        for layer in layers:\n",
    "            nn.init.normal_(layer.weight,mean=0.,std=0.1)\n",
    "            nn.init.constant_(layer.bias,0.)\n",
    "\n",
    "    def forward(self,s):\n",
    "      # Use of tanh activation function is recommanded : bounded [-1,1],\n",
    "      # gives some non-linearity, and tends to give some stability.\n",
    "        x = torch.tanh(self.fc1(s))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        # mu action output of the NN.\n",
    "        mu = self.mu(x)\n",
    "        #log_sigma action output of the NN\n",
    "        log_sigma = self.sigma(x)\n",
    "        sigma = torch.exp(log_sigma)\n",
    "        return mu,sigma\n",
    "\n",
    "    def choose_action(self,s):\n",
    "      # Choose action in the continuous action space using normal distribution\n",
    "      # defined by mu and sigma of each actions returned by the NN.\n",
    "        s = torch.from_numpy(np.array(s).astype(np.float32)).unsqueeze(0).to(self.device)\n",
    "        mu,sigma = self.forward(s)\n",
    "        Pi = self.distribution(mu,sigma)\n",
    "        return Pi.sample().cpu().numpy().squeeze(0)\n",
    "    \n",
    "    def save_model(self):\n",
    "        torch.save(self.state_dict(), self.checkpoint_file)\n",
    "        \n",
    "    def load_model(self):\n",
    "        self.load_state_dict(torch.load(self.checkpoint_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic class : Used to estimate V(state) the state value function through a NN.\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, N_S, chkpt_dir):\n",
    "      # Initialize NN structure.\n",
    "        super(Critic,self).__init__()\n",
    "        self.fc1 = nn.Linear(N_S,64)\n",
    "        self.fc2 = nn.Linear(64,64)\n",
    "        self.fc3 = nn.Linear(64,1)\n",
    "        self.fc3.weight.data.mul_(0.1) # 초기 weight에 0.1을 곱해주면서 학습을 더 안정적으로 할 수 있도록(tanh, sigmoid를 사용할 경우 많이 쓰는 방식)\n",
    "        self.fc3.bias.data.mul_(0.0) # bias tensor의 모든 원소를 0으로 설정\n",
    "        \n",
    "        self.checkpoint_dir = chkpt_dir\n",
    "        self.checkpoint_file = os.path.join(self.checkpoint_dir, '_critic')\n",
    "        \n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def set_init(self,layers):\n",
    "      # Initialize weight and bias according to a normal distrib mean 0 and sd 0.1.\n",
    "        for layer in layers:\n",
    "            nn.init.normal_(layer.weight,mean=0.,std=0.1)\n",
    "            nn.init.constant_(layer.bias,0.)\n",
    "\n",
    "    def forward(self,s):\n",
    "      # Use of tanh activation function is recommanded.\n",
    "        x = torch.tanh(self.fc1(s))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        values = self.fc3(x)\n",
    "        return values\n",
    "    \n",
    "    def save_model(self):\n",
    "        torch.save(self.state_dict(), self.checkpoint_file)\n",
    "        \n",
    "    def load_model(self):\n",
    "        self.load_state_dict(torch.load(self.checkpoint_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, N_S, N_A, log_dir):\n",
    "        self.log_dir = log_dir\n",
    "        \n",
    "        self.actor_net = Actor(N_S, N_A, log_dir)\n",
    "        self.critic_net = Critic(N_S, log_dir)\n",
    "        self.actor_optim = optim.Adam(self.actor_net.parameters(), lr=1e-4)\n",
    "        self.critic_optim = optim.Adam(self.critic_net.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "        self.critic_loss_func = torch.nn.MSELoss()\n",
    "        \n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def train(self, memory):\n",
    "        states, actions, rewards, masks = [], [], [], []\n",
    "        \n",
    "        for m in memory:\n",
    "            states.append(m[0])\n",
    "            actions.append(m[1])\n",
    "            rewards.append(m[2])\n",
    "            masks.append(m[3])\n",
    "        \n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(np.array(actions), dtype=torch.float32).to(self.device)\n",
    "        rewards = torch.tensor(np.array(rewards), dtype=torch.float32).to(self.device)\n",
    "        masks = torch.tensor(np.array(masks), dtype=torch.float32).to(self.device)\n",
    "\n",
    "        # Use critic network defined in Model.py\n",
    "        # This function enables to get the current state value V(S).\n",
    "        values = self.critic_net(states)\n",
    "        # Get advantage.\n",
    "        returns,advants = self.get_gae(rewards,masks,values)\n",
    "        #Get old mu and std.\n",
    "        old_mu,old_std = self.actor_net(states)\n",
    "        #Get the old distribution.\n",
    "        pi = self.actor_net.distribution(old_mu,old_std)\n",
    "        #Compute old policy.\n",
    "        old_log_prob = pi.log_prob(actions).sum(1,keepdim=True)\n",
    "\n",
    "        # Everything happens here\n",
    "        n = len(states)\n",
    "        arr = np.arange(n)\n",
    "        for epoch in range(1):\n",
    "            np.random.shuffle(arr)\n",
    "            for i in range(n//batch_size):\n",
    "                b_index = arr[batch_size*i:batch_size*(i+1)]\n",
    "                b_states = states[b_index]\n",
    "                b_advants = advants[b_index].unsqueeze(1)\n",
    "                b_actions = actions[b_index]\n",
    "                b_returns = returns[b_index].unsqueeze(1)\n",
    "\n",
    "                #New parameter of the policy distribution by action.\n",
    "                mu,std = self.actor_net(b_states)\n",
    "                pi = self.actor_net.distribution(mu,std)\n",
    "                new_prob = pi.log_prob(b_actions).sum(1,keepdim=True)\n",
    "                old_prob = old_log_prob[b_index].detach()\n",
    "                #Regularisation fixed KL : does not work as good as following clipping strategy\n",
    "                # empirically.\n",
    "                # KL_penalty = self.kl_divergence(old_mu[b_index],old_std[b_index],mu,std)\n",
    "                ratio = torch.exp(new_prob-old_prob)\n",
    "\n",
    "                surrogate_loss = ratio*b_advants\n",
    "                values = self.critic_net(b_states)\n",
    "                # MSE Loss : (State action value - State value)^2\n",
    "                critic_loss = self.critic_loss_func(values,b_returns)\n",
    "                # critic_loss = critic_loss - beta*KL_penalty\n",
    "\n",
    "                self.critic_optim.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                self.critic_optim.step()\n",
    "                #Clipping strategy\n",
    "                ratio = torch.clamp(ratio,1.0-epsilon,1.0+epsilon)\n",
    "                clipped_loss =ratio*b_advants\n",
    "                # Actual loss\n",
    "                actor_loss = -torch.min(surrogate_loss,clipped_loss).mean()\n",
    "                \n",
    "                walker_xvel = torch.tensor([get_walker_x_velocity(state) for state in b_states], dtype=torch.float32).to(self.device)\n",
    "                actor_loss = augmented_objective(actor_loss, walker_xvel, 3, 20)\n",
    "\n",
    "                #Now that we have the loss, we can do the backward propagation to learn : everything is here.\n",
    "                self.actor_optim.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_optim.step()\n",
    "                \n",
    "    # Get the Kullback - Leibler divergence: Measure of the diff btwn new and old policy:\n",
    "    # Could be used for the objective function depending on the strategy that needs to be\n",
    "    # teste.\n",
    "    def kl_divergence(self,old_mu,old_sigma,mu,sigma):\n",
    "\n",
    "        old_mu = old_mu.detach()\n",
    "        old_sigma = old_sigma.detach()\n",
    "\n",
    "        kl = torch.log(old_sigma) - torch.log(sigma) + (old_sigma.pow(2) + (old_mu - mu).pow(2)) / \\\n",
    "             (2.0 * sigma.pow(2)) - 0.5\n",
    "        return kl.sum(1, keepdim=True)\n",
    "    \n",
    "    # Advantage estimation:\n",
    "    def get_gae(self,rewards, masks, values):\n",
    "        rewards = torch.Tensor(rewards).to(self.device)\n",
    "        masks = torch.Tensor(masks).to(self.device)\n",
    "        #Create an equivalent fullfilled of 0.\n",
    "        returns = torch.zeros_like(rewards).to(self.device)\n",
    "        advants = torch.zeros_like(rewards).to(self.device)\n",
    "        #Init\n",
    "        running_returns = 0\n",
    "        previous_value = 0\n",
    "        running_advants = 0\n",
    "        #Here we compute A_t the advantage.\n",
    "        for t in reversed(range(0, len(rewards))):\n",
    "            # Here we compute the discounted returns. Gamma is the discount factor.\n",
    "            running_returns = rewards[t] + gamma * running_returns * masks[t]\n",
    "            #computes the difference between the estimated value at time step t (values.data[t]) and the discounted next value.\n",
    "            running_tderror = rewards[t] + gamma * previous_value * masks[t] - values.data[t]\n",
    "            # Compute advantage\n",
    "            running_advants = running_tderror + gamma * lambd * running_advants * masks[t]\n",
    "\n",
    "            returns[t] = running_returns\n",
    "            previous_value = values.data[t]\n",
    "            advants[t] = running_advants\n",
    "        #Normalization to stabilize final advantage of the history to now.\n",
    "        advants = (advants - advants.mean()) / advants.std()\n",
    "        return returns, advants\n",
    "\n",
    "    def save(self, filename):\n",
    "        filename = str(filename)\n",
    "        torch.save(self.actor_net.state_dict(), filename + \"_actor\")\n",
    "        torch.save(self.critic_net.state_dict(), filename + \"_critic\")\n",
    "        torch.save(self.actor_optim.state_dict(), filename + \"_actor_optimizer\")\n",
    "        torch.save(self.critic_optim.state_dict(), filename + \"_critic_optimizer\")\n",
    "\n",
    "    def load(self, filename):\n",
    "        filename = str(filename)\n",
    "        self.actor_net.load_state_dict(torch.load(filename + \"_actor\"))\n",
    "        self.critic_net.load_state_dict(torch.load(filename + \"_critic\"))\n",
    "        self.actor_optim.load_state_dict(torch.load(filename + \"_actor_optimizer\"))\n",
    "        self.critic_optim.load_state_dict(torch.load(filename + \"_critic_optimizer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of a class to normalize the states\n",
    "class Normalize:\n",
    "    def __init__(self, N_S, chkpt_dir):\n",
    "        self.mean = np.zeros((N_S,))\n",
    "        self.std = np.zeros((N_S, ))\n",
    "        self.stdd = np.zeros((N_S, ))\n",
    "        self.n = 0\n",
    "        \n",
    "        self.checkpoint_dir = chkpt_dir\n",
    "        self.checkpoint_file = os.path.join(self.checkpoint_dir, '_normalize')\n",
    "        \n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = np.asarray(x)\n",
    "        self.n += 1\n",
    "        if self.n == 1:\n",
    "            self.mean = x\n",
    "        else:\n",
    "            old_mean = self.mean.copy()\n",
    "            self.mean = old_mean + (x - old_mean) / self.n\n",
    "            self.stdd = self.stdd + (x - old_mean) * (x - self.mean)\n",
    "        if self.n > 1:\n",
    "            self.std = np.sqrt(self.stdd / (self.n - 1))\n",
    "        else:\n",
    "            self.std = self.mean\n",
    "\n",
    "        x = x - self.mean\n",
    "        x = x / (self.std + 1e-8)\n",
    "        x = np.clip(x, -5, +5)\n",
    "        return x\n",
    "    \n",
    "    def update(self, x):\n",
    "        self.mean = np.mean(x, axis=0)\n",
    "        self.std = np.std(x, axis=0) + 1e-8\n",
    "    \n",
    "    def save_params(self):\n",
    "        np.save(self.checkpoint_file, {'mean': self.mean, 'std': self.std})\n",
    "\n",
    "    def load_params(self):\n",
    "        params = np.load(self.checkpoint_file, allow_pickle=True).item()\n",
    "        self.mean = params['mean']\n",
    "        self.std = params['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(env, model, episodes=10):\n",
    "    scores = []\n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            env.render()\n",
    "            state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
    "            action = model.actor_net.choose_action(state)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "        scores.append(total_reward)\n",
    "        print(f\"Episode {episode + 1}: Total Reward: {total_reward}\")\n",
    "    print(f\"Average Reward over {episodes} episodes: {np.mean(scores)}\")\n",
    "    env.close()\n",
    "    \n",
    "def get_walker_x_velocity(state):\n",
    "    x_vel = state[8]\n",
    "    return x_vel\n",
    "\n",
    "def logarithmic_barrier(state, constraint_max):\n",
    "    return -torch.log(-(state - constraint_max))\n",
    "\n",
    "def augmented_objective(actor_loss, state, constraint_max, t):\n",
    "    constraint_barrier = logarithmic_barrier(state, constraint_max) / t\n",
    "    return actor_loss + constraint_barrier.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109 episode score is -0.10, average_xvel is 0.046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/100000 [00:03<110:32:47,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210 episode score is 0.33, average_xvel is 0.029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/100000 [00:07<98:59:16,  3.56s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315 episode score is 0.53, average_xvel is 0.027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/100000 [00:10<95:19:39,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421 episode score is 1.35, average_xvel is 0.078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/100000 [00:13<93:41:34,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "523 episode score is 0.81, average_xvel is 0.032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/100000 [00:17<92:21:36,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "617 episode score is 2.13, average_xvel is 0.100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/100000 [00:20<91:32:36,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "718 episode score is 1.69, average_xvel is 0.072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/100000 [00:23<91:17:10,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "806 episode score is 3.34, average_xvel is 0.142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/100000 [00:26<91:05:50,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "895 episode score is 4.46, average_xvel is 0.199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/100000 [00:30<91:00:37,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "988 episode score is 3.83, average_xvel is 0.149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/100000 [00:33<90:56:10,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1070 episode score is 5.34, average_xvel is 0.195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/100000 [00:36<90:43:14,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1141 episode score is 6.82, average_xvel is 0.205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/100000 [00:39<91:14:45,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1213 episode score is 7.70, average_xvel is 0.249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 13/100000 [00:43<91:00:25,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1279 episode score is 8.69, average_xvel is 0.235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 14/100000 [00:46<91:04:55,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1332 episode score is 9.78, average_xvel is 0.179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 15/100000 [00:49<90:54:05,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1382 episode score is 16.20, average_xvel is 0.373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 16/100000 [00:53<91:00:23,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1431 episode score is 19.89, average_xvel is 0.458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 17/100000 [00:56<91:00:48,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1477 episode score is 21.03, average_xvel is 0.426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 18/100000 [00:59<91:05:58,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1519 episode score is 22.74, average_xvel is 0.387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 19/100000 [01:02<90:49:18,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1558 episode score is 30.70, average_xvel is 0.529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 20/100000 [01:06<90:58:10,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1595 episode score is 33.89, average_xvel is 0.523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 21/100000 [01:09<91:20:30,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1627 episode score is 43.69, average_xvel is 0.593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 22/100000 [01:12<91:15:24,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1657 episode score is 43.32, average_xvel is 0.475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 23/100000 [01:16<91:43:03,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1675 episode score is 113.08, average_xvel is 0.944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 24/100000 [01:19<91:21:24,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1693 episode score is 123.20, average_xvel is 0.966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 25/100000 [01:22<91:18:10,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1710 episode score is 120.56, average_xvel is 0.777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 26/100000 [01:26<93:30:04,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1726 episode score is 118.12, average_xvel is 0.691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 27/100000 [01:29<92:48:11,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1742 episode score is 133.53, average_xvel is 0.779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 28/100000 [01:32<93:01:00,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1756 episode score is 176.43, average_xvel is 0.946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 29/100000 [01:36<92:45:25,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1772 episode score is 127.82, average_xvel is 0.671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 30/100000 [01:39<92:16:23,  3.32s/it]"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    env = gym.make('Walker2d-v4', render_mode='rgb_array')\n",
    "\n",
    "    #Number of state and action\n",
    "    N_S = env.observation_space.shape[0]\n",
    "    N_A = env.action_space.shape[0]\n",
    "\n",
    "    # Random seed initialization\n",
    "    # env.seed(500)\n",
    "    # torch.manual_seed(500)\n",
    "    # np.random.seed(500)\n",
    "\n",
    "    # Run the Ppo class\n",
    "    frames = []\n",
    "    ppo = PPO(N_S, N_A, log_dir)\n",
    "    # ppo.actor_net.load_model(\"../runs/20240708_11-19-08/ppo/100000/\")\n",
    "    # ppo.critic_net.load_model(\"../runs/20240708_11-19-08/ppo/100000/\")\n",
    "    \n",
    "    # Normalisation for stability, fast convergence... always good to do.\n",
    "    normalize = Normalize(N_S, log_dir)\n",
    "    episodes = 0\n",
    "    eva_episodes = 0\n",
    "    episode_data = []\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    for iter in tqdm(range(Iter)):\n",
    "        memory = deque()\n",
    "        scores = []\n",
    "        steps = 0\n",
    "        xvel = []\n",
    "        while steps < 2048: #Horizon\n",
    "            episodes += 1\n",
    "            state, _ = env.reset()\n",
    "            s = normalize(state)\n",
    "            score = 0\n",
    "            for _ in range(MAX_STEP):\n",
    "                steps += 1\n",
    "                #Choose an action: detailed in PPO.py\n",
    "                # The action is a numpy array of 17 elements. It means that in the 17 possible directions of action we have a specific value in the continuous space.\n",
    "                # Exemple : the first coordinate correspond to the Torque applied on the hinge in the y-coordinate of the abdomen: this is continuous space.\n",
    "                a = ppo.actor_net.choose_action(s)\n",
    "                # print(f\"{YELLOW}walker velocity: {RESET}\", s[8]) # 3\n",
    "                #Environnement reaction to the action : There is a reaction in the 376 elements that characterize the space :\n",
    "                # Exemple : the first coordinate of the states is the z-coordinate of the torso (centre) and using env.step(a), we get the reaction of this state and\n",
    "                # of all the other ones after the action has been made.\n",
    "                s_ , r ,truncated, terminated ,info = env.step(a)\n",
    "                s_ = normalize(s_)\n",
    "                done = truncated or terminated\n",
    "\n",
    "                # Do we continue or do we terminate an episode?\n",
    "                mask = (1-done)*1\n",
    "                memory.append([s,a,r,mask])\n",
    "                # print('s: ', s)\n",
    "                # print('a: ', a)\n",
    "                # print('r: ', r)\n",
    "                # print('mask: ', mask)\n",
    "                xvel.append(s[8])\n",
    "                score += r\n",
    "                s = s_\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "            # with open('log_' + args.env_name  + '.txt', 'a') as outfile:\n",
    "            #     outfile.write('\\t' + str(episodes)  + '\\t' + str(score) + '\\n')\n",
    "            scores.append(score)\n",
    "        score_avg = np.mean(scores)\n",
    "        xvel_avg = np.mean(xvel)\n",
    "        print('{} episode score is {:.2f}, average_xvel is {:.3f}'.format(episodes, score_avg, xvel_avg))\n",
    "        episode_data.append([iter + 1, score_avg])\n",
    "        if (iter + 1) % save_freq == 0:\n",
    "            save_flag = True\n",
    "\n",
    "            if save_flag:\n",
    "                ppo.actor_net.save_model()\n",
    "                ppo.critic_net.save_model()\n",
    "                normalize.save_params()\n",
    "                print(f\"{GREEN} >> Successfully saved models! {RESET}\")\n",
    "                # path = log_dir + \"ppo/\" + str((iter + 1)) + \"/\"\n",
    "                # os.makedirs(path, exist_ok=True)\n",
    "                # if not os.path.exists(path):\n",
    "                #     os.makedirs(path)\n",
    "                # ppo.save(path)\n",
    "\n",
    "                np.save(log_dir + \"reward.npy\", episode_data)\n",
    "                save_flag = False\n",
    "\n",
    "        ppo.train(memory)\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('average score: ', score_avg)\n",
    "print('average xvel:  ', xvel_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo.actor_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
