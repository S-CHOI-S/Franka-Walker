{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "\n",
    "import time\n",
    "import psutil\n",
    "import datetime\n",
    "import subprocess\n",
    "# import torch\n",
    "import torchvision\n",
    "from tensorboard import program\n",
    "import webbrowser\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from color_code import *\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#learning rate backward propagation NN action\n",
    "lr_actor = 0.0003\n",
    "#learning rate backward propagation NN state value estimation\n",
    "lr_critic = 0.0003\n",
    "#Number of Learning Iteration we want to perform\n",
    "Iter = 100000\n",
    "#Number max of step to realise in one episode. \n",
    "MAX_STEP = 1000\n",
    "#How rewards are discounted.\n",
    "gamma =0.98\n",
    "#How do we stabilize variance in the return computation.\n",
    "lambd = 0.95\n",
    "#batch to train on\n",
    "batch_size = 64\n",
    "# Do we want high change to be taken into account.\n",
    "epsilon = 0.2\n",
    "#weight decay coefficient in ADAM for state value optim.\n",
    "l2_rate = 0.001\n",
    "\n",
    "save_freq = 100\n",
    "\n",
    "save_flag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor class: Used to choose actions of a continuous action space.\n",
    "class Actor(nn.Module):\n",
    "  def __init__(self, N_S, N_A, chkpt_dir):\n",
    "    # Initialize NN structure.\n",
    "    super(Actor,self).__init__()\n",
    "    self.fc1 = nn.Linear(N_S,64)\n",
    "    self.fc2 = nn.Linear(64,64)\n",
    "    self.sigma = nn.Linear(64,N_A)\n",
    "    self.mu = nn.Linear(64,N_A)\n",
    "    self.mu.weight.data.mul_(0.1)\n",
    "    self.mu.bias.data.mul_(0.0)\n",
    "    # This approach use gaussian distribution to decide actions. Could be\n",
    "    # something else.\n",
    "    self.distribution = torch.distributions.Normal\n",
    "    \n",
    "    self.checkpoint_dir = chkpt_dir\n",
    "    self.checkpoint_file = os.path.join(self.checkpoint_dir, '_actor')\n",
    "    \n",
    "    self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    self.to(self.device)\n",
    "\n",
    "  def set_init(self,layers):\n",
    "    # Initialize weight and bias according to a normal distrib mean 0 and sd 0.1.\n",
    "    for layer in layers:\n",
    "      nn.init.normal_(layer.weight,mean=0.,std=0.1)\n",
    "      nn.init.constant_(layer.bias,0.)\n",
    "\n",
    "  def forward(self,state):\n",
    "    # Use of tanh activation function is recommanded : bounded [-1,1],\n",
    "    # gives some non-linearity, and tends to give some stability.\n",
    "    x = torch.tanh(self.fc1(state))\n",
    "    x = torch.tanh(self.fc2(x))\n",
    "    # mu action output of the NN.\n",
    "    mu = self.mu(x)\n",
    "    #log_sigma action output of the NN\n",
    "    log_sigma = self.sigma(x)\n",
    "    sigma = torch.exp(log_sigma)\n",
    "    return mu,sigma\n",
    "\n",
    "  def choose_action(self,state):\n",
    "    # Choose action in the continuous action space using normal distribution\n",
    "    # defined by mu and sigma of each actions returned by the NN.\n",
    "    state = torch.from_numpy(np.array(state).astype(np.float32)).unsqueeze(0).to(self.device)\n",
    "    mu,sigma = self.forward(state)\n",
    "    Pi = self.distribution(mu,sigma)\n",
    "    return Pi.sample().cpu().numpy().squeeze(0)\n",
    "  \n",
    "  def save_model(self):\n",
    "    torch.save(self.state_dict(), self.checkpoint_file)\n",
    "      \n",
    "  def load_model(self, load_model_dir=None):\n",
    "    if load_model_dir is None:\n",
    "        load_model_dir = self.checkpoint_file\n",
    "    self.load_state_dict(torch.load(load_model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic class : Used to estimate V(state) the state value function through a NN.\n",
    "class Critic(nn.Module):\n",
    "  def __init__(self, N_S, chkpt_dir):\n",
    "    # Initialize NN structure.\n",
    "    super(Critic,self).__init__()\n",
    "    self.fc1 = nn.Linear(N_S,64)\n",
    "    self.fc2 = nn.Linear(64,64)\n",
    "    self.fc3 = nn.Linear(64,1)\n",
    "    self.fc3.weight.data.mul_(0.1) # 초기 weight에 0.1을 곱해주면서 학습을 더 안정적으로 할 수 있도록(tanh, sigmoid를 사용할 경우 많이 쓰는 방식)\n",
    "    self.fc3.bias.data.mul_(0.0) # bias tensor의 모든 원소를 0으로 설정\n",
    "    \n",
    "    self.checkpoint_dir = chkpt_dir\n",
    "    self.checkpoint_file = os.path.join(self.checkpoint_dir, '_critic')\n",
    "    \n",
    "    self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    self.to(self.device)\n",
    "\n",
    "  def set_init(self,layers):\n",
    "    # Initialize weight and bias according to a normal distrib mean 0 and sd 0.1.\n",
    "    for layer in layers:\n",
    "      nn.init.normal_(layer.weight,mean=0.,std=0.1)\n",
    "      nn.init.constant_(layer.bias,0.)\n",
    "\n",
    "  def forward(self,state):\n",
    "    # Use of tanh activation function is recommanded.\n",
    "    x = torch.tanh(self.fc1(state))\n",
    "    x = torch.tanh(self.fc2(x))\n",
    "    values = self.fc3(x)\n",
    "    return values\n",
    "  \n",
    "  def save_model(self):\n",
    "    torch.save(self.state_dict(), self.checkpoint_file)\n",
    "      \n",
    "  def load_model(self, load_model_dir=None):\n",
    "    if load_model_dir is None:\n",
    "      load_model_dir = self.checkpoint_file\n",
    "    self.load_state_dict(torch.load(load_model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multihead Cost Value Function\n",
    "class MultiheadCostValueFunction(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_heads):\n",
    "        super(MultiheadCostValueFunction, self).__init__()\n",
    "        \n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.heads = nn.ModuleList([nn.Linear(hidden_dim, 1) for _ in range(num_heads)])\n",
    "        \n",
    "        self._init_weights()\n",
    "        \n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shared_layers(x)\n",
    "        return torch.cat([head(x) for head in self.heads], dim=1)\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for layer in self.shared_layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                layer.weight.data.mul_(0.1)\n",
    "                layer.bias.data.mul_(0.0)\n",
    "        for head in self.heads:\n",
    "            head.weight.data.mul_(0.1)\n",
    "            head.bias.data.mul_(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Algorithm with Constraints   \n",
    "class PPO:\n",
    "    def __init__(self, N_S, N_A, log_dir, num_constraints=2, cstrnt_limit=None):\n",
    "        self.log_dir = log_dir\n",
    "        \n",
    "        self.actor_net = Actor(N_S, N_A, log_dir)\n",
    "        self.critic_net = Critic(N_S, log_dir)\n",
    "        self.multihead_net = MultiheadCostValueFunction(N_S, 64, num_constraints)\n",
    "        \n",
    "        self.actor_optim = optim.Adam(self.actor_net.parameters(), lr=1e-4)\n",
    "        self.critic_optim = optim.Adam(self.critic_net.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "        self.multihead_optim = optim.Adam(self.multihead_net.parameters(), lr=1e-3)\n",
    "        self.critic_loss_func = torch.nn.MSELoss()\n",
    "        \n",
    "        if len(cstrnt_limit) != num_constraints:\n",
    "            print(f\"{RED}[ERROR] Cstrnts' info is mismatch! Please check the num of cstrnt{RESET}\")\n",
    "            sys.exit()\n",
    "        # elif num_constraints == 0:\n",
    "            \n",
    "        else:\n",
    "            self.constraint_limits = cstrnt_limit\n",
    "            self.adaptive_constraints = cstrnt_limit\n",
    "        \n",
    "        self.alpha = 0.1\n",
    "        self.t = 20\n",
    "        \n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def train(self, memory):\n",
    "        states, actions, rewards, masks = [], [], [], []\n",
    "        costs = [[] for _ in range(len(memory[0][4]))]\n",
    "        \n",
    "        for m in memory:\n",
    "            states.append(m[0])\n",
    "            actions.append(m[1])\n",
    "            rewards.append(m[2])\n",
    "            masks.append(m[3])\n",
    "            for i, cost in enumerate(m[4]):\n",
    "                costs[i].append(cost)\n",
    "        \n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(np.array(actions), dtype=torch.float32).to(self.device)\n",
    "        rewards = torch.tensor(np.array(rewards), dtype=torch.float32).to(self.device)\n",
    "        masks = torch.tensor(np.array(masks), dtype=torch.float32).to(self.device)\n",
    "        costs = [torch.tensor(np.array(cost), dtype=torch.float32).to(self.device) for cost in costs]\n",
    "\n",
    "        values = self.critic_net(states)\n",
    "        cost_values = self.multihead_net(states)\n",
    "        returns, advants, cost_advants = self.compute_cost_advantages(rewards, masks, values, cost_values)\n",
    "        old_mu, old_std = self.actor_net(states)\n",
    "        pi = self.actor_net.distribution(old_mu, old_std)\n",
    "        old_log_prob = pi.log_prob(actions).sum(1, keepdim=True)\n",
    "        \n",
    "        self.adaptive_constraints = self.adaptive_constraint_thresholding(cost_values, self.constraint_limits)\n",
    "\n",
    "        n = len(states)\n",
    "        arr = np.arange(n)\n",
    "        for epoch in range(1):\n",
    "            np.random.shuffle(arr)\n",
    "            for i in range(n // batch_size):\n",
    "                b_index = arr[batch_size * i:batch_size * (i + 1)]\n",
    "                b_states = states[b_index]\n",
    "                b_advants = advants[b_index].unsqueeze(1)\n",
    "                b_actions = actions[b_index]\n",
    "                b_returns = returns[b_index].unsqueeze(1)\n",
    "\n",
    "                mu, std = self.actor_net(b_states)\n",
    "                \n",
    "                pi = self.actor_net.distribution(mu, std)\n",
    "                new_prob = pi.log_prob(b_actions).sum(1, keepdim=True)\n",
    "                old_prob = old_log_prob[b_index].detach()\n",
    "                ratio = torch.exp(new_prob - old_prob)\n",
    "\n",
    "                surrogate_loss = ratio * b_advants\n",
    "                values = self.critic_net(b_states)\n",
    "                critic_loss = self.critic_loss_func(values, b_returns)\n",
    "\n",
    "                self.critic_optim.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                self.critic_optim.step()\n",
    "\n",
    "                ratio = torch.clamp(ratio, 1.0 - epsilon, 1.0 + epsilon)\n",
    "                clipped_loss = ratio * b_advants\n",
    "                actor_loss = -torch.min(surrogate_loss, clipped_loss).mean() # PPO\n",
    "\n",
    "                cost_values_estimates = self.multihead_net(b_states)\n",
    "                for idx, (cost_value, cost_advant, adaptive_limit) in enumerate(zip(cost_values_estimates.t(), cost_advants, self.adaptive_constraints)):\n",
    "                    actor_loss = self.augmented_objective(actor_loss, cost_value, adaptive_limit, self.t, b_advants, old_mu[b_index], old_std[b_index], mu, std)\n",
    "            \n",
    "                self.actor_optim.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_optim.step()\n",
    "\n",
    "                cost_value_loss = sum([torch.nn.functional.mse_loss(est.squeeze(), val[b_index]) for est, val in zip(cost_values_estimates.t(), costs)])\n",
    "\n",
    "                self.multihead_optim.zero_grad()\n",
    "                cost_value_loss.backward()\n",
    "                self.multihead_optim.step()\n",
    "                \n",
    "    # Get the Kullback - Leibler divergence: Measure of the diff btwn new and old policy:\n",
    "    # Could be used for the objective function depending on the strategy that needs to be\n",
    "    def kl_divergence(self, old_mu, old_sigma, mu, sigma):\n",
    "\n",
    "        old_mu = old_mu.detach()\n",
    "        old_sigma = old_sigma.detach()\n",
    "\n",
    "        kl = torch.log(old_sigma) - torch.log(sigma) + (old_sigma.pow(2) + (old_mu - mu).pow(2)) / \\\n",
    "             (2.0 * sigma.pow(2)) - 0.5\n",
    "        return kl.sum(1, keepdim=True)\n",
    "    \n",
    "    # Advantage estimation:\n",
    "    def get_gae(self,rewards, masks, values):\n",
    "        rewards = torch.Tensor(rewards).to(self.device)\n",
    "        masks = torch.Tensor(masks).to(self.device)\n",
    "        # Create an equivalent fullfilled of 0.\n",
    "        returns = torch.zeros_like(rewards).to(self.device)\n",
    "        advants = torch.zeros_like(rewards).to(self.device)\n",
    "        # Init\n",
    "        running_returns = 0\n",
    "        previous_value = 0\n",
    "        running_advants = 0\n",
    "        # Here we compute A_t the advantage.\n",
    "        for t in reversed(range(0, len(rewards))):\n",
    "            # Here we compute the discounted returns. Gamma is the discount factor.\n",
    "            running_returns = rewards[t] + gamma * running_returns * masks[t]\n",
    "            #computes the difference between the estimated value at time step t (values.data[t]) and the discounted next value.\n",
    "            running_tderror = rewards[t] + gamma * previous_value * masks[t] - values.data[t]\n",
    "            # Compute advantage\n",
    "            running_advants = running_tderror + gamma * lambd * running_advants * masks[t]\n",
    "\n",
    "            returns[t] = running_returns\n",
    "            previous_value = values.data[t]\n",
    "            advants[t] = running_advants\n",
    "        # Normalization to stabilize final advantage of the history to now.\n",
    "        advants = (advants - advants.mean()) / advants.std()\n",
    "        return returns, advants\n",
    "    \n",
    "    def compute_cost_advantages(self, rewards, masks, values, cost_values):\n",
    "        returns, advantages = self.get_gae(rewards, masks, values)\n",
    "        cost_advantages = []\n",
    "        for cost_value in cost_values:\n",
    "            _, cost_advantage = self.get_gae(cost_value.squeeze(), masks, cost_value.squeeze())\n",
    "            cost_advantages.append(cost_advantage)\n",
    "        return returns, advantages, cost_advantages\n",
    "    \n",
    "    def logarithmic_barrier(self, cost, constraint_max):\n",
    "        indicator = torch.where((cost - constraint_max) <= 0, (cost - constraint_max).clone().detach(), torch.tensor(0, device=cost.device))\n",
    "        return -torch.log(-indicator)\n",
    "\n",
    "    def augmented_objective(self, actor_loss, cost, constraint_max, t, advants, old_mu, old_sigma, mu, sigma):\n",
    "        constraint_barrier = self.logarithmic_barrier(cost, constraint_max) / t\n",
    "        kl_divergence = self.kl_divergence(old_mu, old_sigma, mu, sigma).mean()\n",
    "        return actor_loss + constraint_barrier.mean() + advants.mean() #+ kl_divergence \n",
    "    \n",
    "    def adaptive_constraint_thresholding(self, cost_values, constraint_limits):\n",
    "        adaptive_limits = []\n",
    "        for cost_value, constraint_limit in zip(cost_values.t(), constraint_limits):\n",
    "            current_cost = cost_value.mean().item()\n",
    "            adaptive_limit = max(constraint_limit, current_cost + self.alpha * constraint_limit)\n",
    "            adaptive_limits.append(adaptive_limit)\n",
    "        return adaptive_limits\n",
    "    \n",
    "    def save(self):\n",
    "        # filename = str(filename)\n",
    "        torch.save(self.actor_optim.state_dict(), self.log_dir + \"_actor_optimizer\")\n",
    "        torch.save(self.critic_optim.state_dict(), self.log_dir + \"_critic_optimizer\")\n",
    "\n",
    "    def load(self, log_dir=None):\n",
    "        # filename = str(filename)\n",
    "        if log_dir == None:\n",
    "            log_dir = self.log_dir\n",
    "        self.actor_optim.load_state_dict(torch.load(log_dir + \"_actor_optimizer\"))\n",
    "        self.critic_optim.load_state_dict(torch.load(log_dir + \"_critic_optimizer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of a class to normalize the states\n",
    "class Normalize:\n",
    "    def __init__(self, N_S, chkpt_dir, train_mode=True):\n",
    "        self.mean = np.zeros((N_S,))\n",
    "        self.std = np.zeros((N_S, ))\n",
    "        self.stdd = np.zeros((N_S, ))\n",
    "        self.n = 0\n",
    "        \n",
    "        self.train_mode = train_mode\n",
    "        \n",
    "        self.checkpoint_dir = chkpt_dir\n",
    "        self.checkpoint_file = os.path.join(self.checkpoint_dir, '_normalize.npy')\n",
    "        \n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = np.asarray(x)\n",
    "        if self.train_mode:\n",
    "            self.n += 1\n",
    "            if self.n == 1:\n",
    "                self.mean = x\n",
    "            else:\n",
    "                old_mean = self.mean.copy()\n",
    "                self.mean = old_mean + (x - old_mean) / self.n\n",
    "                self.stdd = self.stdd + (x - old_mean) * (x - self.mean)\n",
    "            if self.n > 1:\n",
    "                self.std = np.sqrt(self.stdd / (self.n - 1))\n",
    "            else:\n",
    "                self.std = self.mean\n",
    "\n",
    "        x = x - self.mean\n",
    "        x = x / (self.std + 1e-8)\n",
    "        x = np.clip(x, -5, +5)\n",
    "        return x\n",
    "    \n",
    "    def update(self, x):\n",
    "        self.mean = np.mean(x, axis=0)\n",
    "        self.std = np.std(x, axis=0) + 1e-8\n",
    "    \n",
    "    def save_params(self):\n",
    "        np.save(self.checkpoint_file, {'mean': self.mean, 'std': self.std})\n",
    "\n",
    "    def load_params(self, load_model_dir=None):\n",
    "        if load_model_dir is None:\n",
    "            load_model_dir = self.checkpoint_file\n",
    "        params = np.load(load_model_dir, allow_pickle=True).item()\n",
    "        self.mean = params['mean']\n",
    "        self.std = params['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = gym.make('Walker2d-v4', render_mode='rgb_array')\n",
    "\n",
    "    N_S = env.observation_space.shape[0]\n",
    "    N_A = env.action_space.shape[0]\n",
    "    \n",
    "    cstrnt1_limit = 0.2 # y angle of the torso\n",
    "    cstrnt2_limit = 0.5 # x vel of the torso\n",
    "\n",
    "    ppo = PPO(N_S, N_A, log_dir, cstrnt_limit=[cstrnt1_limit, cstrnt2_limit])\n",
    "    normalize = Normalize(N_S, log_dir)\n",
    "    \n",
    "    # ppo.actor_net.load_model('../runs/20240715_19-42-33/_actor')\n",
    "    # ppo.actor_net.eval()\n",
    "    # ppo.critic_net.load_model('../runs/20240715_19-42-33/_critic')\n",
    "    # ppo.critic_net.eval()\n",
    "    # ppo.load('../runs/20240715_19-42-33/')\n",
    "    # normalize.load_params('../runs/20240715_19-42-33/_normalize.npy')\n",
    "\n",
    "    episodes = 0\n",
    "    episode_data = []\n",
    "    constraint_data = []\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    for iter in tqdm(range(Iter)):\n",
    "        memory = deque()\n",
    "        scores = []\n",
    "        steps = 0\n",
    "        cstrnt1 = []\n",
    "        cstrnt2 = []\n",
    "        while steps < 2048: #Horizon\n",
    "            episodes += 1\n",
    "            state, _ = env.reset()\n",
    "            state = normalize(state)\n",
    "            score = 0\n",
    "            for _ in range(MAX_STEP):\n",
    "                steps += 1\n",
    "                \n",
    "                action = ppo.actor_net.choose_action(state)\n",
    "                next_state, reward, truncated, terminated, info = env.step(action)\n",
    "                next_state = normalize(next_state)\n",
    "                done = truncated or terminated\n",
    "                \n",
    "                cost1 = next_state[1]\n",
    "                cost2 = next_state[8]\n",
    "\n",
    "                mask = (1-done)*1\n",
    "                memory.append([state, action, reward, mask, [cost1, cost2]])\n",
    "                cstrnt1.append(state[1])\n",
    "                cstrnt2.append(state[8])\n",
    "                score += reward\n",
    "                state = next_state\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            scores.append(score)\n",
    "        \n",
    "        score_avg = np.mean(scores)\n",
    "        cstrnt1_avg = np.mean(cstrnt1)\n",
    "        cstrnt2_avg = np.mean(cstrnt2)\n",
    "        \n",
    "        if (cstrnt1_avg <= ppo.adaptive_constraints[0]) & (cstrnt2_avg <= ppo.adaptive_constraints[1]):\n",
    "            print(f\"\\n{episodes} episode score is {score_avg:.2f}, cstrnt1 is {GREEN}{cstrnt1_avg:.3f}/ {ppo.adaptive_constraints[0]:.3f}{RESET}, cstrnt2 is {GREEN}{cstrnt2_avg:.3f}/ {ppo.adaptive_constraints[1]:.3f}{RESET}\")\n",
    "        elif (cstrnt1_avg <= ppo.adaptive_constraints[0]) & (cstrnt2_avg > ppo.adaptive_constraints[1]):\n",
    "            print(f\"\\n{episodes} episode score is {score_avg:.2f}, cstrnt1 is {GREEN}{cstrnt1_avg:.3f}/ {ppo.adaptive_constraints[0]:.3f}{RESET}, cstrnt2 is {RED}{cstrnt2_avg:.3f}/ {ppo.adaptive_constraints[1]:.3f}{RESET}\")\n",
    "        elif (cstrnt1_avg > ppo.adaptive_constraints[0]) & (cstrnt2_avg <= ppo.adaptive_constraints[1]):\n",
    "            print(f\"\\n{episodes} episode score is {score_avg:.2f}, cstrnt1 is {RED}{cstrnt1_avg:.3f}/ {ppo.adaptive_constraints[0]:.3f}{RESET}, cstrnt2 is {GREEN}{cstrnt2_avg:.3f}/ {ppo.adaptive_constraints[1]:.3f}{RESET}\")\n",
    "        else:\n",
    "            print(f\"\\n{episodes} episode score is {score_avg:.2f}, cstrnt1 is {RED}{cstrnt1_avg:.3f}/ {ppo.adaptive_constraints[0]:.3f}{RESET}, cstrnt2 is {RED}{cstrnt2_avg:.3f}/ {ppo.adaptive_constraints[1]:.3f}{RESET}\")\n",
    "        \n",
    "        episode_data.append([iter + 1, score_avg])\n",
    "        constraint_data.append([iter + 1, ppo.adaptive_constraints[0], cstrnt1_avg, ppo.adaptive_constraints[1], cstrnt2_avg])\n",
    "        \n",
    "        if (iter + 1) % save_freq == 0:\n",
    "            save_flag = True\n",
    "\n",
    "            if save_flag:\n",
    "                ppo.actor_net.save_model()\n",
    "                ppo.critic_net.save_model()\n",
    "                ppo.save()\n",
    "                normalize.save_params()\n",
    "                print(f\"{GREEN} >> Successfully saved models! {RESET}\")\n",
    "\n",
    "                np.save(log_dir + \"reward.npy\", episode_data)\n",
    "                np.save(log_dir + \"constraint.npy\", constraint_data)\n",
    "                print(f\"{GREEN} >> Successfully saved reward & constraint data! {RESET}\")\n",
    "                \n",
    "                save_flag = False\n",
    "\n",
    "        ppo.train(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H-%M-%S\")\n",
    "    log_dir = f\"../runs/{current_time}/\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    writer = SummaryWriter(log_dir)\n",
    "    print(f\"{YELLOW}[MODEL/TENSORBOARD]{RESET} The data will be saved in {YELLOW}{log_dir}{RESET} directory!\")\n",
    "\n",
    "    # tb = program.TensorBoard()\n",
    "    # tb.configure(argv=[None, '--logdir', f\"../runs/franka_cabinet/{current_time}\", '--port', '6300'])\n",
    "    # url = tb.launch()\n",
    "    # webbrowser.open_new(url)\n",
    "    \n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
