{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76fe0bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "\n",
    "from walker import PPO, Normalize\n",
    "\n",
    "RED = \"\\033[31m\"\n",
    "GREEN = \"\\033[32m\"\n",
    "YELLOW = \"\\033[33m\"\n",
    "BLUE = \"\\033[34m\"\n",
    "MAGENTA = \"\\033[35m\"\n",
    "CYAN = \"\\033[36m\"\n",
    "RESET = \"\\033[0m\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b097465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actor(\n",
       "  (fc1): Linear(in_features=17, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (sigma): Linear(in_features=64, out_features=6, bias=True)\n",
       "  (mu): Linear(in_features=64, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize environment\n",
    "env = gym.make('Walker2d-v4', render_mode='rgb_array')\n",
    "\n",
    "log_dir = \"../runs/20240712_02-38-11/\"\n",
    "\n",
    "# Number of state and action\n",
    "N_S = env.observation_space.shape[0]\n",
    "N_A = env.action_space.shape[0]\n",
    "\n",
    "# Initialize PPO model\n",
    "ppo = PPO(N_S, N_A, log_dir)\n",
    "normalize = Normalize(N_S, log_dir)\n",
    "\n",
    "# Load the saved model\n",
    "ppo.actor_net.load_model()\n",
    "ppo.actor_net.eval()\n",
    "normalize.load_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50a5fded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial state:  [-0.70710428  0.70710538 -0.70710392 -0.7070993   0.70708507 -0.70710467\n",
      "  0.70710514 -0.70709734  0.70710566 -0.70710407 -0.70710551  0.7070915\n",
      " -0.70709833 -0.7071049  -0.70710268  0.70710484 -0.70710183]\n",
      "episode:  0 \tscore:  -36.67559178923674\n",
      "initial state:  [ 4.83876177e-01  9.49394189e-01 -1.87067416e-01 -1.13418201e-01\n",
      "  7.32700303e-01 -2.08805995e+00 -2.53796650e-01 -1.27467210e-01\n",
      "  1.57910588e+00  6.46092782e-01  9.55926646e-01 -1.74592316e-02\n",
      "  1.34176342e-03  1.50983192e-01 -5.52326491e-02  9.83781270e-04\n",
      "  1.32822805e-01]\n",
      "episode:  1 \tscore:  -10.560587481711615\n",
      "initial state:  [ 0.7032588   0.97823902 -1.00089282 -0.37876452 -0.06091392 -2.67230465\n",
      " -0.52421287 -0.31443171  1.49370478  0.68153022  0.95315263 -0.0066872\n",
      " -0.00659636  0.12432343 -0.03438324  0.00657912  0.06273744]\n",
      "episode:  2 \tscore:  -12.611072934886023\n",
      "initial state:  [ 7.77006200e-01  9.97541230e-01 -9.69830708e-01 -6.01462015e-01\n",
      " -2.24659944e-01 -3.34123353e+00 -7.59112263e-01 -3.79797689e-01\n",
      "  1.51476968e+00  6.94423397e-01  1.00984093e+00 -1.20905590e-02\n",
      "  4.32447279e-04  1.07396027e-01 -3.40421696e-02 -1.01601640e-03\n",
      "  3.22696707e-02]\n",
      "episode:  3 \tscore:  -61.09887396182053\n",
      "initial state:  [ 6.82167834e-01  9.98369542e-01 -8.39977949e-01 -5.99944379e-01\n",
      " -2.60994232e-01 -3.14173174e+00 -6.57758509e-01 -2.43915436e-01\n",
      "  1.45912455e+00  7.03606800e-01  1.00260789e+00 -2.14487161e-02\n",
      "  2.03934677e-04  7.06102495e-02 -4.15808634e-02 -4.46129559e-03\n",
      " -1.00469038e-02]\n",
      "episode:  4 \tscore:  -46.33549093369209\n",
      "initial state:  [ 6.42282668e-01  1.04542222e+00 -1.13830969e+00 -3.06732562e-01\n",
      " -2.95975658e-01 -1.67239639e+00 -7.39970303e-01 -1.19988345e-01\n",
      "  1.49020872e+00  7.02512987e-01  9.32739827e-01 -4.34829610e-03\n",
      "  1.28204499e-02  6.02580972e-02 -3.20307986e-02 -5.69460332e-03\n",
      "  7.66882044e-04]\n",
      "episode:  5 \tscore:  -53.852801866516714\n",
      "initial state:  [ 0.61901931  1.05551905 -0.78282112 -0.1809143  -0.31966093 -1.11499045\n",
      " -0.69377624 -0.02619135  1.49755934  0.70431903  0.90344882  0.0187739\n",
      "  0.00924935  0.04398471 -0.04571536 -0.00626892  0.00839173]\n",
      "episode:  6 \tscore:  -34.47355081393936\n",
      "initial state:  [ 0.58713994  1.08149204 -0.56162051 -0.32159223 -0.33509924 -1.18955614\n",
      " -0.62415454  0.04484635  1.51395493  0.6997734   0.91398456  0.02478184\n",
      "  0.01072936  0.03414904 -0.04393343 -0.007451    0.01858259]\n",
      "episode:  7 \tscore:  -19.27596646740855\n",
      "initial state:  [ 0.63083366  1.07310625 -0.97903929 -0.20047637 -0.36237256 -1.35355377\n",
      " -0.24146819 -0.04154629  1.51146229  0.70577907  0.93154513  0.01839767\n",
      "  0.01285029  0.0388287  -0.0358623  -0.005059    0.01783854]\n",
      "episode:  8 \tscore:  -44.3045843213802\n",
      "initial state:  [ 0.65600504  1.05805414 -1.15661949 -0.30552253 -0.36246277 -1.83655392\n",
      " -0.21033667 -0.02134617  1.51761553  0.70289692  0.9505278   0.02008596\n",
      "  0.01383993  0.05047648 -0.04434266 -0.0052457   0.03408643]\n",
      "episode:  9 \tscore:  -51.722789777088174\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "state, _ = env.reset()\n",
    "state = normalize(state)\n",
    "\n",
    "test_total_reward = 0\n",
    "test_episodes = 10  # Number of episodes to test\n",
    "for episode_id in range(test_episodes):\n",
    "    state, _ = env.reset()\n",
    "    state = normalize(state)\n",
    "    # state = np.zeros(17)\n",
    "    print('initial state: ', state)\n",
    "    score = 0\n",
    "    for _ in range(1000):\n",
    "        action = ppo.actor_net.choose_action(state)\n",
    "        # print(f\"{YELLOW}walker velocity: {RESET}\", state[8])\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        state = normalize(state)\n",
    "        score += reward\n",
    "\n",
    "        if done:\n",
    "            env.reset()\n",
    "            break\n",
    "    \n",
    "    print(\"episode: \", episode_id, \"\\tscore: \", score)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a6e40d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0e093f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
