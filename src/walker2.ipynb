{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_276222/3373988866.py:71: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  mini_batch = torch.tensor(s_batch, dtype=torch.float), torch.tensor(a_batch, dtype=torch.float), \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of episode :20, avg score : -1604.9, optmization step: 400\n",
      "# of episode :40, avg score : -1634.7, optmization step: 800\n",
      "# of episode :60, avg score : -1530.4, optmization step: 1200\n",
      "# of episode :80, avg score : -1312.2, optmization step: 1600\n",
      "# of episode :100, avg score : -1137.0, optmization step: 2100\n",
      "# of episode :120, avg score : -1180.1, optmization step: 2500\n",
      "# of episode :140, avg score : -1064.0, optmization step: 2900\n",
      "# of episode :160, avg score : -1136.9, optmization step: 3300\n",
      "# of episode :180, avg score : -1155.8, optmization step: 3700\n",
      "# of episode :200, avg score : -1176.1, optmization step: 4200\n",
      "# of episode :220, avg score : -1076.3, optmization step: 4600\n",
      "# of episode :240, avg score : -1055.4, optmization step: 5000\n",
      "# of episode :260, avg score : -1035.4, optmization step: 5400\n",
      "# of episode :280, avg score : -1051.8, optmization step: 5800\n",
      "# of episode :300, avg score : -1021.7, optmization step: 6300\n",
      "# of episode :320, avg score : -1006.2, optmization step: 6700\n",
      "# of episode :340, avg score : -886.6, optmization step: 7100\n",
      "# of episode :360, avg score : -791.2, optmization step: 7500\n",
      "# of episode :380, avg score : -822.6, optmization step: 7900\n",
      "# of episode :400, avg score : -704.6, optmization step: 8300\n",
      "# of episode :420, avg score : -782.3, optmization step: 8800\n",
      "# of episode :440, avg score : -547.1, optmization step: 9200\n",
      "# of episode :460, avg score : -603.1, optmization step: 9600\n",
      "# of episode :480, avg score : -508.0, optmization step: 10000\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "learning_rate  = 0.0003\n",
    "gamma           = 0.9\n",
    "lmbda           = 0.9\n",
    "eps_clip        = 0.2\n",
    "K_epoch         = 10\n",
    "rollout_len    = 3\n",
    "buffer_size    = 10\n",
    "minibatch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PPO, self).__init__()\n",
    "        self.data = []\n",
    "        \n",
    "        self.fc1   = nn.Linear(3,128)\n",
    "        self.fc_mu = nn.Linear(128,1)\n",
    "        self.fc_std  = nn.Linear(128,1)\n",
    "        self.fc_v = nn.Linear(128,1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.optimization_step = 0\n",
    "\n",
    "    def pi(self, x, softmax_dim = 0):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        mu = 2.0*torch.tanh(self.fc_mu(x))\n",
    "        std = F.softplus(self.fc_std(x))\n",
    "        return mu, std\n",
    "    \n",
    "    def v(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        v = self.fc_v(x)\n",
    "        return v\n",
    "      \n",
    "    def put_data(self, transition):\n",
    "        self.data.append(transition)\n",
    "        \n",
    "    def make_batch(self):\n",
    "        s_batch, a_batch, r_batch, s_prime_batch, prob_a_batch, done_batch = [], [], [], [], [], []\n",
    "        data = []\n",
    "\n",
    "        for j in range(buffer_size):\n",
    "            for i in range(minibatch_size):\n",
    "                rollout = self.data.pop()\n",
    "                s_lst, a_lst, r_lst, s_prime_lst, prob_a_lst, done_lst = [], [], [], [], [], []\n",
    "\n",
    "                for transition in rollout:\n",
    "                    s, a, r, s_prime, prob_a, done = transition\n",
    "                    \n",
    "                    s_lst.append(s)\n",
    "                    a_lst.append([a])\n",
    "                    r_lst.append([r])\n",
    "                    s_prime_lst.append(s_prime)\n",
    "                    prob_a_lst.append([prob_a])\n",
    "                    done_mask = 0 if done else 1\n",
    "                    done_lst.append([done_mask])\n",
    "\n",
    "                s_batch.append(s_lst)\n",
    "                a_batch.append(a_lst)\n",
    "                r_batch.append(r_lst)\n",
    "                s_prime_batch.append(s_prime_lst)\n",
    "                prob_a_batch.append(prob_a_lst)\n",
    "                done_batch.append(done_lst)\n",
    "                    \n",
    "            mini_batch = torch.tensor(s_batch, dtype=torch.float), torch.tensor(a_batch, dtype=torch.float), \\\n",
    "                          torch.tensor(r_batch, dtype=torch.float), torch.tensor(s_prime_batch, dtype=torch.float), \\\n",
    "                          torch.tensor(done_batch, dtype=torch.float), torch.tensor(prob_a_batch, dtype=torch.float)\n",
    "            data.append(mini_batch)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def calc_advantage(self, data):\n",
    "        data_with_adv = []\n",
    "        for mini_batch in data:\n",
    "            s, a, r, s_prime, done_mask, old_log_prob = mini_batch\n",
    "            with torch.no_grad():\n",
    "                td_target = r + gamma * self.v(s_prime) * done_mask\n",
    "                delta = td_target - self.v(s)\n",
    "            delta = delta.numpy()\n",
    "\n",
    "            advantage_lst = []\n",
    "            advantage = 0.0\n",
    "            for delta_t in delta[::-1]:\n",
    "                advantage = gamma * lmbda * advantage + delta_t[0]\n",
    "                advantage_lst.append([advantage])\n",
    "            advantage_lst.reverse()\n",
    "            advantage = torch.tensor(advantage_lst, dtype=torch.float)\n",
    "            data_with_adv.append((s, a, r, s_prime, done_mask, old_log_prob, td_target, advantage))\n",
    "\n",
    "        return data_with_adv\n",
    "\n",
    "        \n",
    "    def train_net(self):\n",
    "        if len(self.data) == minibatch_size * buffer_size:\n",
    "            data = self.make_batch()\n",
    "            data = self.calc_advantage(data)\n",
    "\n",
    "            for i in range(K_epoch):\n",
    "                for mini_batch in data:\n",
    "                    s, a, r, s_prime, done_mask, old_log_prob, td_target, advantage = mini_batch\n",
    "\n",
    "                    mu, std = self.pi(s, softmax_dim=1)\n",
    "                    dist = Normal(mu, std)\n",
    "                    log_prob = dist.log_prob(a)\n",
    "                    ratio = torch.exp(log_prob - old_log_prob)  # a/b == exp(log(a)-log(b))\n",
    "\n",
    "                    surr1 = ratio * advantage\n",
    "                    surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage\n",
    "                    loss = -torch.min(surr1, surr2) + F.smooth_l1_loss(self.v(s) , td_target)\n",
    "\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.mean().backward()\n",
    "                    nn.utils.clip_grad_norm_(self.parameters(), 1.0)\n",
    "                    self.optimizer.step()\n",
    "                    self.optimization_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = gym.make('Pendulum-v1')\n",
    "    model = PPO()\n",
    "    score = 0.0\n",
    "    print_interval = 20\n",
    "    rollout = []\n",
    "\n",
    "    for n_epi in range(10000):\n",
    "        s, _ = env.reset()\n",
    "        done = False\n",
    "        count = 0\n",
    "        while count < 200 and not done:\n",
    "            for t in range(rollout_len):\n",
    "                mu, std = model.pi(torch.from_numpy(s).float())\n",
    "                dist = Normal(mu, std)\n",
    "                a = dist.sample()\n",
    "                log_prob = dist.log_prob(a)\n",
    "                s_prime, r, done, truncated, info = env.step([a.item()])\n",
    "\n",
    "                rollout.append((s, a, r/10.0, s_prime, log_prob.item(), done))\n",
    "                if len(rollout) == rollout_len:\n",
    "                    model.put_data(rollout)\n",
    "                    rollout = []\n",
    "\n",
    "                s = s_prime\n",
    "                score += r\n",
    "                count += 1\n",
    "\n",
    "            model.train_net()\n",
    "\n",
    "        if n_epi%print_interval==0 and n_epi!=0:\n",
    "            print(\"# of episode :{}, avg score : {:.1f}, optmization step: {}\".format(n_epi, score/print_interval, model.optimization_step))\n",
    "            score = 0.0\n",
    "\n",
    "    env.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
